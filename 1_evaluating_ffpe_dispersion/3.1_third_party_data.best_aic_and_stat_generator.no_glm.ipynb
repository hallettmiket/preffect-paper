{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0debac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-03T20:09:18.464231Z",
     "iopub.status.busy": "2024-05-03T20:09:18.463980Z",
     "iopub.status.idle": "2024-05-03T20:09:19.436123Z",
     "shell.execute_reply": "2024-05-03T20:09:19.435359Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import sys\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.genmod.families import NegativeBinomial, Gamma\n",
    "from statsmodels.discrete.count_model import ZeroInflatedNegativeBinomialP\n",
    "\n",
    "from scipy.stats import expon, nbinom, norm, poisson\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# now the DCIS count data is found in an RDA file, which we apparently read using 'pyreadr'\n",
    "import pyreadr\n",
    "\n",
    "import os\n",
    "\n",
    "# thread issues\n",
    "os.environ['OMP_NUM_THREADS'] = '12'  # Limit to 1 thread\n",
    "os.environ['MKL_NUM_THREADS'] = '12'  # Limit to 1 thread for MKL (if used)\n",
    "os.environ['NUMEXPR_NUM_THREADS'] = '12'  # Limit to 1 thread for NumExpr (if used)\n",
    "\n",
    "\n",
    "\n",
    "# to convert Ensemble to Refseq gene names\n",
    "gene_convert = pyreadr.read_r('/path/to/gene_info/ensemble_to_refseq_gene_name_table.rds')\n",
    "gene_convert = gene_convert[None]\n",
    "id_to_name = {gene_id: gene_name for gene_id, gene_name in zip(gene_convert[\"gene_id\"], gene_convert[\"gene_name\"])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245e5697",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-03T20:09:19.440477Z",
     "iopub.status.busy": "2024-05-03T20:09:19.440172Z",
     "iopub.status.idle": "2024-05-03T20:09:19.444096Z",
     "shell.execute_reply": "2024-05-03T20:09:19.443549Z"
    }
   },
   "outputs": [],
   "source": [
    "### Parameters ###\n",
    "\n",
    "# whether or not we're doing outlier removal using trimmed means\n",
    "trim_means_flag = True\n",
    "trim_percent = 1 # 1% usually gets rid of most extreme outliers\n",
    "\n",
    "# genes must be expressed in this % of patients (between 0-1)\n",
    "express_percent_limit = 0.0 # set to 0 if you want patient stats (all genes with at least 1 read), set to 0.2 if we want AIC stats of genes with >20% expression\n",
    "\n",
    "# library adjust (using fractional method)\n",
    "adjust_for_lib = False\n",
    "\n",
    "# calculate AIC distance\n",
    "calc_AIC_dist = False # False saves time when running the full program\n",
    "\n",
    "# a flag if we want to just do \"no ZI\" or \"NB vs ZINB only\"\n",
    "NB_ZINB_only = False # Only comparing NB to ZINB [trim_percent should be low, maybe even zero]\n",
    "\n",
    "# trim will remove zeroes, so I don't think we should activate trim when doing NB/ZINB comparison\n",
    "if (NB_ZINB_only == True):\n",
    "    trim_percent = 0\n",
    "\n",
    "no_ZI_AICs = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ea7391",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-03T20:09:19.448011Z",
     "iopub.status.busy": "2024-05-03T20:09:19.447811Z",
     "iopub.status.idle": "2024-05-03T20:09:19.455322Z",
     "shell.execute_reply": "2024-05-03T20:09:19.454758Z"
    }
   },
   "outputs": [],
   "source": [
    "# non-AIC related functions used in this program    \n",
    "\n",
    "# computation of gene average, fraction of zeroes, and library size\n",
    "def dataset_stats_generator(df, draw_zero_distribution = False, dataset_name = \"\"):\n",
    "\n",
    "\n",
    "\n",
    "    num_genes = df.shape[0]\n",
    "    num_samples = df.shape[1]\n",
    "    # Compute the metrics for each row\n",
    "    col_sums = df.sum(axis = 0)\n",
    "\n",
    "    fraction_zero_samples = (df == 0).sum(axis=0) / num_genes\n",
    "    fraction_zero_genes = (df == 0).sum(axis=1) / num_samples\n",
    "    row_means = df.mean(axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    if (draw_zero_distribution):\n",
    "        plt.hist(fraction_zero_genes, bins=100, color='blue', alpha=0.7)\n",
    "        plt.xlabel('Fraction of Zeroes (Genes)')\n",
    "        plt.ylabel('Count')\n",
    "        plt.title(\"Fraction of Zeroes per Gene\")\n",
    "        plt.show()\n",
    "\n",
    "        plt.hist(fraction_zero_samples, bins=100, color='blue', alpha=0.7)\n",
    "        plt.xlabel('Fraction of Zeroes (Samples)')\n",
    "        plt.ylabel('Count')\n",
    "        plt.title(\"Fraction of Zeroes per Sample\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        min_data = np.min(row_means + 0.0000001)\n",
    "        max_data = np.max(row_means + 1)\n",
    "        print(min_data, max_data)\n",
    "\n",
    "        # Generate log-spaced bins\n",
    "        bins = np.logspace(np.log10(min_data), np.log10(max_data), 500)\n",
    "\n",
    "\n",
    "\n",
    "        plt.hist(row_means, bins=bins, color='blue', alpha=0.7, log=True)\n",
    "        plt.xlabel('Means of Transcript Counts')\n",
    "        plt.ylabel('Counts (log-scaled)')\n",
    "        # plt.xscale('log')\n",
    "        # plt.yscale('log')\n",
    "        # plt.xlim(left=1)\n",
    "        plt.title(\"Distribution of Mean Counts Per Transcript: \" + dataset_name)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "        plt.hist(row_means[row_means >= 10000], bins=100, color='blue', alpha=0.7)\n",
    "        plt.xlabel('Means of Gene Expression (means >=10,000 only)')\n",
    "        plt.ylabel('Counts (log-scaled)')\n",
    "        plt.yscale('log')\n",
    "        plt.title(\"Distribution of Mean Counts Per Gene: \" + dataset_name)\n",
    "        plt.show()\n",
    "      \n",
    "    # get the average of these \n",
    "    avg_library_size = np.round(np.sum(col_sums) / num_samples, decimals = 0)\n",
    "    avg_zeroes = np.round(np.sum(fraction_zero_samples) / num_samples, decimals = 3)\n",
    "    avg_mean_expression = np.round(np.mean(row_means), decimals = 3)\n",
    "    stdev_mean_expression = np.round(np.std(row_means), decimals = 3)\n",
    "    \n",
    "    # print(\"Avg Library Size\", \"Avg Fraction Zeroes\", \"Avg Mean Expression\")\n",
    "    return avg_library_size, avg_zeroes, avg_mean_expression, stdev_mean_expression\n",
    "\n",
    "# Simulating some data for illustration\n",
    "#data = np.random.negative_binomial(10, 0.5, 1000)\n",
    "\n",
    "def fit_to_nb_plot(data, plotrange = 30):\n",
    "\n",
    "    # Estimating parameters directly from data\n",
    "    mean = np.mean(data)\n",
    "    var = np.var(data)\n",
    "    p = 1 - (mean / var)\n",
    "    n = mean * (1 - p) / p\n",
    "\n",
    "    # Plotting\n",
    "    plt.hist(data, bins=range(plotrange), align='left', density=True, alpha=0.6, color='g')\n",
    "    plt.plot(bins[:-1], nbinom.pmf(bins[:-1], n, p), 'ro-', lw=2)\n",
    "    plt.title(\"Negative Binomial Fit\")\n",
    "    plt.show()\n",
    "\n",
    "# adjust for library sizes\n",
    "def library_adjust(data):\n",
    "    if (adjust_for_lib):\n",
    "        library_size = data.sum(axis=0)\n",
    "        \n",
    "        cleaned_matrix = np.round((data /library_size)*10000000)\n",
    "        return cleaned_matrix\n",
    "    else:\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fea31f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-03T20:09:19.458855Z",
     "iopub.status.busy": "2024-05-03T20:09:19.458717Z",
     "iopub.status.idle": "2024-05-03T20:09:19.463514Z",
     "shell.execute_reply": "2024-05-03T20:09:19.462978Z"
    }
   },
   "outputs": [],
   "source": [
    "# functions to compute ZINB\n",
    "\n",
    "def zinb_loglike(params, counts):\n",
    "    mu, theta, pi = params\n",
    "    p = 1 / (1 + mu/theta)\n",
    "    n = mu * p / (1 - p)\n",
    "    loglik_pois = nbinom.logpmf(counts, n, p)\n",
    "    loglik_zero = np.log(pi + (1 - pi) * np.exp(nbinom.logpmf(0, n, p)))\n",
    "    loglik = np.where(counts == 0, loglik_zero, np.log(1 - pi) + loglik_pois)\n",
    "    return -np.sum(loglik)\n",
    "\n",
    "def calculate_aic(loglik, k):\n",
    "    return 2*k - 2*loglik\n",
    "\n",
    "def fit_zinb_and_calculate_aic(counts):\n",
    "    \n",
    "    initial_params = np.array([np.mean(counts), np.var(counts), 0.9])\n",
    "    bounds = [(0, None), (0, None), (0, 1)]\n",
    "    result = minimize(zinb_loglike, initial_params, args=(counts), bounds=bounds)\n",
    "    mu, theta, pi = result.x\n",
    "    loglik = -result.fun\n",
    "    \n",
    "    k = 3  # Number of parameters\n",
    "    aic = calculate_aic(loglik, k)\n",
    "    return mu, theta, pi, aic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee79dff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-03T20:09:19.467288Z",
     "iopub.status.busy": "2024-05-03T20:09:19.467150Z",
     "iopub.status.idle": "2024-05-03T20:09:19.471553Z",
     "shell.execute_reply": "2024-05-03T20:09:19.471022Z"
    }
   },
   "outputs": [],
   "source": [
    "# functions to compute ZIP\n",
    "def zip_loglike(params, counts):\n",
    "    mu, pi = params\n",
    "    loglik_pois = poisson.logpmf(counts, mu)\n",
    "    loglik_zero = np.log(pi + (1 - pi) * np.exp(poisson.logpmf(0, mu)))\n",
    "    loglik = np.where(counts == 0, loglik_zero, np.log(1 - pi) + loglik_pois)\n",
    "    return -np.sum(loglik)\n",
    "\n",
    "def calculate_aic(loglik, k):\n",
    "    return 2*k - 2*loglik\n",
    "\n",
    "def fit_zip_and_calculate_aic(counts):\n",
    "    \n",
    "    initial_params = np.array([np.mean(counts), 0.9])\n",
    "    bounds = [(0, None), (0, 1)]\n",
    "    result = minimize(zip_loglike, initial_params, args=(counts), bounds=bounds)\n",
    "    mu, pi = result.x\n",
    "    loglik = -result.fun\n",
    "    k = 2  # Number of parameters for ZIP model\n",
    "    aic = calculate_aic(loglik, k)\n",
    "    return mu, pi, aic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4cf92b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-03T20:09:19.475215Z",
     "iopub.status.busy": "2024-05-03T20:09:19.475069Z",
     "iopub.status.idle": "2024-05-03T20:09:19.484929Z",
     "shell.execute_reply": "2024-05-03T20:09:19.484368Z"
    }
   },
   "outputs": [],
   "source": [
    "def compare_distributions(aic_values):\n",
    "    distribution_types = {\n",
    "        \"NB\": aic_values[\"NB\"],\n",
    "        \"Gaussian\": aic_values[\"Gaussian\"],\n",
    "        \"Poisson\": aic_values[\"Poisson\"],\n",
    "        \"Exponential\": aic_values[\"Exponential\"],\n",
    "        \"ZIP\": aic_values[\"Zero-Inflated Poisson\"],\n",
    "        \"ZINB\": aic_values[\"Zero-Inflated Negative Binomial\"]\n",
    "    }\n",
    "\n",
    "    min_aic_type = min(distribution_types, key=distribution_types.get)\n",
    "    min_aic_value = distribution_types[min_aic_type]\n",
    "\n",
    "    return min_aic_type, min_aic_value\n",
    "\n",
    "\n",
    "\n",
    "# this is the function that computes AIC for all distributions (Gaussian, Exponential, Negative Binomial, ZIP, ZINB)\n",
    "# and reports which distribution is lowest\n",
    "# row - a vector of expressions\n",
    "def manual_aic(row):\n",
    "    \n",
    "    row = np.round(row) # it must be count data\n",
    "\n",
    "    # trimmed mean to remove outliers\n",
    "    n = len(row)\n",
    "    \n",
    "    if (trim_means_flag):\n",
    "        elements_to_trim = int(np.floor(trim_percent / 100.0 * n))  \n",
    "        sorted_data = np.sort(row)\n",
    "        \n",
    "        if (elements_to_trim > 0):\n",
    "            row = sorted_data[elements_to_trim:-elements_to_trim]\n",
    "        else: \n",
    "            row = sorted_data\n",
    "\n",
    "    if (sum(row) <= 0):\n",
    "        return \"ZEROES\"\n",
    "\n",
    "    eps = 0.0000000000001\n",
    "\n",
    "    X = sm.add_constant(np.ones(len(row)))\n",
    "\n",
    "    # Exponential parameters\n",
    "    lambda_exp = 1 / np.mean(row)\n",
    "    log_likelihood_exp = np.sum(expon.logpdf(row, scale=1/lambda_exp))\n",
    "    aic_exp = 2*1 - 2*log_likelihood_exp  # 1 parameter for exponential\n",
    "\n",
    "    # log -> linear -> delog\n",
    "    # using StatsModels to fit to a Gamma (it doesn't have exponential)\n",
    "    #model_exponential_approx = sm.GLM(row, X, family=Gamma()).fit()\n",
    "    #print('AIC for Gamma:', model_exponential_approx.aic)\n",
    "    #aic_exp = model_exponential_approx.aic\n",
    "\n",
    "\n",
    "\n",
    "    # Compute AIC to NB manually \n",
    "    mu_sample = np.mean(row)\n",
    "    var_sample = np.var(row)\n",
    "    if (mu_sample == var_sample):\n",
    "        var_sample = var_sample + eps\n",
    "    r_estimated = mu_sample**2 / (var_sample - mu_sample)\n",
    "    \n",
    "    if (mu_sample + r_estimated) == 0:\n",
    "        r_estimated = r_estimated + eps\n",
    "    p_estimated = r_estimated / (mu_sample + r_estimated)\n",
    "    #log_likelihood_nb = np.sum(nbinom.logpmf(row, r_estimated, p_estimated))\n",
    "    #aic_nb = 2*2 - 2*log_likelihood_nb  # 2 parameters for NB\n",
    "    \n",
    "    #print(\"NB AIC SciPy\", aic_nb_orig)\n",
    "\n",
    "    # StatsModels method to compute fit to NB\n",
    "    #X = sm.add_constant(np.ones(len(row)))\n",
    "    #model_nb = sm.GLM(row, X, family=NegativeBinomial()).fit(disp=0)\n",
    "    #aic_nb = model_nb.aic\n",
    "    #print(model_nb.summary())\n",
    "    #print(\"NB AIC GLM\", aic_nb)\n",
    "\n",
    "    res = sm.NegativeBinomial(row, X).fit(start_params=[1,1], disp=0)\n",
    "    \n",
    "    const = res.params[0]\n",
    "    alpha = res.params[1]\n",
    "\n",
    "    mu = np.exp(const)\n",
    "    p = 1/(1+np.exp(const)*alpha)\n",
    "    n = np.exp(const)*p/(1-p)\n",
    "\n",
    "    nb_theta = mu * (1 - p) / p\n",
    "\n",
    "    aic_nb = res.aic\n",
    "\n",
    "    # ZINB parameters\n",
    "    mu, zinb_theta, zinb_pi, aic = fit_zinb_and_calculate_aic(row)\n",
    "    aic_zinb = aic\n",
    "    \n",
    "\n",
    "    if (NB_ZINB_only is False):\n",
    "        # AIC of Gaussian following IRLS (IRLS)\n",
    "        model_gaussian = sm.GLM(row, X, family=sm.families.Gaussian()).fit(disp=0)\n",
    "        aic_gauss = model_gaussian.aic\n",
    "\n",
    "\n",
    "        # Poisson parameters (all methods give the same AIC)\n",
    "        model_poisson = sm.Poisson(row, X).fit(disp=0)\n",
    "        aic_pois = model_poisson.aic\n",
    "\n",
    "        # Usage\n",
    "        mu, pi, aic = fit_zip_and_calculate_aic(row)\n",
    "        aic_zip = aic\n",
    "        #print(f\"mu: {mu}, pi: {pi}, AIC: {aic}\")\n",
    "    else:\n",
    "        aic_zip = 100000000\n",
    "        aic_gauss = 100000000\n",
    "        aic_pois = 100000000\n",
    "\n",
    "    # sometimes ZIP and ZINB can be NaN if there are no zeroes\n",
    "    # NB can also become NaN if Mean > Variance (I think)\n",
    "    # just in case, lets add this check for all of them\n",
    "    aic_scores = {'aic_zip': aic_zip, 'aic_zinb': aic_zinb, 'aic_nb': aic_nb, 'aic_pois': aic_pois, 'aic_gauss': aic_gauss, 'aic_exp': aic_exp}\n",
    "\n",
    "    for key in aic_scores:\n",
    "        if np.isnan(aic_scores[key]) | np.isinf(aic_scores[key]):\n",
    "            aic_scores[key] = 100000000\n",
    "\n",
    "    aic_zip, aic_zinb, aic_nb, aic_pois, aic_gauss, aic_exp = aic_scores.values()\n",
    "        \n",
    "    # in certain analyses, we might only want to look at certain distributions\n",
    "    # so we will make the AIC score high for those we don't care about\n",
    "    if (NB_ZINB_only == True):\n",
    "        aic_zip, aic_pois, aic_gauss, aic_exp = 10000000, 10000000, 10000000, 10000000\n",
    "    if (no_ZI_AICs == True):\n",
    "        aic_zip, aic_zinb = 10000000, 10000000\n",
    "    \n",
    "    # print(aic_nb, aic_gauss, aic_pois, aic_exp)\n",
    "    best_distribution, best_aic = compare_distributions({\n",
    "        \"NB\": aic_nb,\n",
    "        \"Gaussian\": aic_gauss,\n",
    "        \"Poisson\": aic_pois,\n",
    "        \"Exponential\": aic_exp,\n",
    "        \"Zero-Inflated Poisson\": aic_zip,\n",
    "        \"Zero-Inflated Negative Binomial\": aic_zinb\n",
    "    })\n",
    "\n",
    "    return best_distribution, nb_theta, zinb_theta, zinb_pi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81954ab3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-03T20:09:19.488604Z",
     "iopub.status.busy": "2024-05-03T20:09:19.488468Z",
     "iopub.status.idle": "2024-05-03T20:09:19.498547Z",
     "shell.execute_reply": "2024-05-03T20:09:19.498005Z"
    }
   },
   "outputs": [],
   "source": [
    "# this version of the function computes the same values but instead returns the \n",
    "# differential between the NB AIC and other distributions\n",
    "\n",
    "# row - a vector of expressions\n",
    "def manual_aic_distfromNB(row):\n",
    "    \n",
    "    row = np.round(row) # make it count data\n",
    "    # New Way - Trimmed Means\n",
    "    # trimmed mean to remove outliers\n",
    "    n = len(row)\n",
    "    \n",
    "    if (trim_means_flag):\n",
    "        elements_to_trim = int(np.floor(trim_percent / 100.0 * n))  \n",
    "        sorted_data = np.sort(row)\n",
    "        \n",
    "        if (elements_to_trim > 0):\n",
    "            row = sorted_data[elements_to_trim:-elements_to_trim]\n",
    "        else: \n",
    "            row = sorted_data\n",
    "\n",
    "    \n",
    "    if (sum(row) <= 0):\n",
    "        return -9999\n",
    "\n",
    "    # Exponential parameters\n",
    "    lambda_exp = 1 / np.mean(row)\n",
    "    log_likelihood_exp = np.sum(expon.logpdf(row, scale=1/lambda_exp))\n",
    "    aic_exp = 2*1 - 2*log_likelihood_exp  # 1 parameter for exponential\n",
    "   \n",
    "\n",
    "    # NB parameters\n",
    "    mu_sample = np.mean(row)\n",
    "    var_sample = np.var(row)\n",
    "    if (mu_sample == var_sample):\n",
    "        var_sample = var_sample + 0.0000000000001\n",
    "    r_estimated = mu_sample**2 / (var_sample - mu_sample)\n",
    "    \n",
    "    if (mu_sample + r_estimated) == 0:\n",
    "        r_estimated = r_estimated + 0.0000000000001\n",
    "    p_estimated = r_estimated / (mu_sample + r_estimated)\n",
    "    log_likelihood_nb = np.sum(nbinom.logpmf(row, r_estimated, p_estimated))\n",
    "    aic_nb = 2*2 - 2*log_likelihood_nb  # 2 parameters for NB\n",
    "\n",
    "    # ZINB parameters\n",
    "    pi_zinb = np.mean(row == 0)\n",
    "    log_likelihood_zeros = np.sum(np.log(pi_zinb) * (row == 0))\n",
    "    log_likelihood_non_zeros = np.sum(np.log(1 - pi_zinb) + nbinom.logpmf(row[row != 0], r_estimated, p_estimated))\n",
    "    log_likelihood_zinb = log_likelihood_zeros + log_likelihood_non_zeros\n",
    "    aic_zinb = 2*3 - 2*log_likelihood_zinb  # 3 parameters for ZINB: pi, r, and p\n",
    "\n",
    "    # Gaussian parameters\n",
    "    mu_gauss = np.mean(row)\n",
    "    sigma_gauss = np.std(row)\n",
    "    log_likelihood_gauss = np.sum(norm.logpdf(row, mu_gauss, sigma_gauss))\n",
    "    aic_gauss = 2*2 - 2*log_likelihood_gauss  # 2 parameters for Gaussian: mu and sigma\n",
    "\n",
    "   \n",
    "\n",
    "    # Poisson parameters\n",
    "    lambda_pois = np.mean(row)\n",
    "    log_likelihood_pois = np.sum(poisson.logpmf(row, lambda_pois))\n",
    "    aic_pois = 2*1 - 2*log_likelihood_pois  # 1 parameter for Poisson: lambda\n",
    "\n",
    "    # ZIP parameters\n",
    "    pi_zip = np.mean(row == 0)\n",
    "    lambda_zip = np.mean(row[row != 0])\n",
    "    log_likelihood_zeros_zip = np.sum(np.log(pi_zip) * (row == 0))\n",
    "    log_likelihood_non_zeros_zip = np.sum(np.log(1 - pi_zip) + poisson.logpmf(row[row != 0], lambda_zip))\n",
    "    log_likelihood_zip = log_likelihood_zeros_zip + log_likelihood_non_zeros_zip\n",
    "    aic_zip = 2*2 - 2*log_likelihood_zip  # 2 parameters for ZIP: pi and lambda\n",
    "    \n",
    "    # ZIP and ZINB can be NaN if there are no zeroes; similarly, NB is NaN if Mean > Variance\n",
    "    # to account for this, lets set them to a very high AIC if this occurs\n",
    "    aic_scores = {'aic_zip': aic_zip, 'aic_zinb': aic_zinb, 'aic_nb': aic_nb, 'aic_pois': aic_pois, 'aic_gauss': aic_gauss, 'aic_exp': aic_exp}\n",
    "\n",
    "    for key in aic_scores:\n",
    "        if np.isnan(aic_scores[key]):\n",
    "            aic_scores[key] = 1000000000\n",
    "\n",
    "    aic_zip, aic_zinb, aic_nb, aic_pois, aic_gauss, aic_exp = aic_scores.values()\n",
    "\n",
    "    if (NB_ZINB_only == True):\n",
    "        aic_zip, aic_pois, aic_gauss, aic_exp = 10000000, 10000000, 10000000, 10000000\n",
    "    if (no_ZI_AICs == True):\n",
    "        aic_zip, aic_zinb = 10000000, 10000000\n",
    "\n",
    "    # Compare AICs and determine best fit\n",
    "    if (aic_nb < aic_pois ) & (aic_nb < aic_gauss) & (aic_nb < aic_exp) & (aic_nb < aic_zip) & (aic_nb < aic_zinb):\n",
    "        return 0\n",
    "    elif (aic_gauss < aic_nb) & (aic_gauss < aic_pois) & (aic_gauss < aic_exp) & (aic_gauss < aic_zip) & (aic_gauss < aic_zinb):\n",
    "        return (aic_nb - aic_gauss)/aic_gauss\n",
    "    elif (aic_exp < aic_nb) & (aic_exp < aic_pois) & (aic_exp < aic_gauss) & (aic_exp < aic_zip) & (aic_exp < aic_zinb):\n",
    "        return (aic_nb - aic_exp)/aic_exp\n",
    "    elif (aic_zinb < aic_nb) & (aic_zinb < aic_pois) & (aic_zinb < aic_gauss) & (aic_zinb < aic_zip) & (aic_zinb < aic_exp):\n",
    "        return (aic_nb - aic_zinb)/aic_zinb\n",
    "    elif (aic_zip < aic_nb) & (aic_zip < aic_pois) & (aic_zip < aic_gauss) & (aic_zip < aic_zinb) & (aic_zip < aic_exp):\n",
    "        return (aic_nb - aic_zip)/aic_zip\n",
    "    else:\n",
    "        return (aic_nb - aic_pois)/aic_pois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf7486b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bcc1d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-03T20:09:19.502234Z",
     "iopub.status.busy": "2024-05-03T20:09:19.502096Z",
     "iopub.status.idle": "2024-05-03T20:09:22.156110Z",
     "shell.execute_reply": "2024-05-03T20:09:22.155428Z"
    }
   },
   "outputs": [],
   "source": [
    "# this is a dataset with 528 FFPE breast cancer samples, sequenced from a HiSeq\n",
    "\n",
    "data = pd.read_csv('/path/to/third_party/GSE167977_third_party_ffpe/GSE167977_Raw_Counts.txt',\n",
    "                  delimiter='\\t')\n",
    "\n",
    "# filter and compute dispersion\n",
    "# dispersion of tumours - All Data\n",
    "tumours_counts = pd.DataFrame(data)\n",
    "\n",
    "tumours_counts.set_index('ensembl_gene_id', inplace=True)\n",
    "#tumours_counts = tumours_counts.drop(tumours_counts.columns[0], axis=1) # column 1\n",
    "tumours_counts = tumours_counts.drop(tumours_counts.columns[-5:], axis=1) # last 5 columns\n",
    "\n",
    "row_means = tumours_counts.mean(axis=1)\n",
    "\n",
    "\n",
    "# adjust for library size (fraction method)\n",
    "# should come before the gene filter\n",
    "tumours_counts_lib_adjust = library_adjust(tumours_counts)\n",
    "\n",
    "fraction_of_zeroes = (tumours_counts_lib_adjust == 0).mean(axis=1)\n",
    "filtered_df = tumours_counts_lib_adjust[fraction_of_zeroes < (1 - express_percent_limit)] # must be expressed to this percentage of patients\n",
    "\n",
    "print(filtered_df.shape)\n",
    "\n",
    "dataset_stats = dataset_stats_generator(filtered_df, draw_zero_distribution=True)\n",
    "print(\"Average Library Size: \", dataset_stats[0])\n",
    "print(\"Fraction of Zeroes: \", dataset_stats[1])\n",
    "print(\"Average Mean Expression: \", dataset_stats[2])\n",
    "print(\"Average Stdev Expression: \", dataset_stats[3])\n",
    "\n",
    "new_tumours_counts = filtered_df.copy()\n",
    "new_tumours_counts['Mean'] = filtered_df.mean(axis=1)\n",
    "new_tumours_counts['StdDev'] = filtered_df.std(axis=1)\n",
    "new_tumours_counts['Min'] = filtered_df.min(axis=1)\n",
    "new_tumours_counts['Max'] = filtered_df.max(axis=1)\n",
    "\n",
    "top_10_means = new_tumours_counts.nlargest(10, 'Mean')\n",
    "\n",
    "# Step 4: Print the standard deviation of the filtered rows\n",
    "print(\"\\nTop 10 expressed genes\")\n",
    "print(top_10_means[['Mean', 'StdDev', 'Min', 'Max']])\n",
    "\n",
    "\n",
    "GSE167977_stdev = [] \n",
    "new_tumours_counts = filtered_df.copy()\n",
    "\n",
    "for i in range(min(10, len(filtered_df))):\n",
    "    # Calculate the mean for each row\n",
    "    row_means = new_tumours_counts.mean(axis=1)\n",
    "    \n",
    "    # Calculate the standard deviation of these means\n",
    "    std_dev_of_means = row_means.std(ddof=0)\n",
    "\n",
    "    # Append the standard deviation to the list\n",
    "    GSE167977_stdev.append(std_dev_of_means)\n",
    "    \n",
    "    # Remove the row with the highest mean\n",
    "    max_mean_index = row_means.idxmax()\n",
    "    new_tumours_counts.drop(index=max_mean_index, inplace=True)\n",
    "\n",
    "# print(dataset_stats)\n",
    "\n",
    "print(GSE167977_stdev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd273d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-03T20:09:22.160823Z",
     "iopub.status.busy": "2024-05-03T20:09:22.160618Z",
     "iopub.status.idle": "2024-05-03T20:09:22.165267Z",
     "shell.execute_reply": "2024-05-03T20:09:22.164823Z"
    }
   },
   "outputs": [],
   "source": [
    "if(calc_AIC_dist):\n",
    "\n",
    "    print(\"GSE167977 - How off NB is to the winning distribution\")\n",
    "    aic_off = filtered_df.apply(manual_aic_distfromNB, axis=1)\n",
    "    print(aic_off)\n",
    "\n",
    "    # I want the average, and I want to eliminate -9999 (that's if all genes are zeroes)\n",
    "    mask = (aic_off != 0) & (aic_off > -9999)\n",
    "\n",
    "    median_off = np.median(aic_off[mask])\n",
    "    print(\"NB AIC is usually: \", median_off)\n",
    "    count_less_than_10 = np.sum(aic_off[mask] < 0.01)\n",
    "    count_greater_equal_10 = np.sum(aic_off[mask] >= 0.01)\n",
    "\n",
    "    print(f\"Count of values < 1%: {count_less_than_10}\")\n",
    "    print(f\"Count of values >= 1%: {count_greater_equal_10}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91bc27d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-03T20:09:22.169136Z",
     "iopub.status.busy": "2024-05-03T20:09:22.168970Z",
     "iopub.status.idle": "2024-05-03T20:20:33.798360Z",
     "shell.execute_reply": "2024-05-03T20:20:33.797148Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for testing\n",
    "#filtered_df = filtered_df.iloc[1:100]\n",
    "\n",
    "# function to compute by row\n",
    "print(\"GSE167977 - Lowest AIC across all genes\")\n",
    "\n",
    "aic_values = filtered_df.apply(manual_aic, axis=1)\n",
    "aic_values, nb_theta_GSE167977, zinb_theta_GSE167977, zinb_pi_GSE167977 = zip(*aic_values)\n",
    "\n",
    "# print(aic_values)\n",
    "\n",
    "AIC_top_rank_GSE167977 = pd.Series(aic_values).value_counts()\n",
    "print(AIC_top_rank_GSE167977)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31373a8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2903f74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-03T20:20:33.803285Z",
     "iopub.status.busy": "2024-05-03T20:20:33.803090Z",
     "iopub.status.idle": "2024-05-03T20:20:33.981974Z",
     "shell.execute_reply": "2024-05-03T20:20:33.981077Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('/path/to/third_party/GSE181466_third_party_ffpe/GSE181466_rsem_genes_matrix-97.txt',\n",
    "                  delimiter='\\t')\n",
    "\n",
    "# patient information splitting is unnecessary, this appears to all be both FFPE and from tumours\n",
    "# there is subtype and age information in the series matrix file, if we're interested\n",
    "\n",
    "# dispersion of tumours - All Data\n",
    "tumours_counts = pd.DataFrame(data)\n",
    "# removing gene column at position 0\n",
    "tumours_counts.set_index('Unnamed: 0', inplace=True)\n",
    "#tumours_counts = tumours_counts.drop(tumours_counts.columns[0], axis=1)\n",
    "# skip genes that are all zeroes, or just one spurrious read somewhere\n",
    "\n",
    "# adjust for library size (fraction method)\n",
    "tumours_counts_libadjust = library_adjust(tumours_counts)\n",
    "\n",
    "fraction_of_zeroes = (tumours_counts_libadjust == 0).mean(axis=1)\n",
    "filtered_df = tumours_counts_libadjust[fraction_of_zeroes < (1 - express_percent_limit)] # must be expressed to this percentage of patients\n",
    "print(filtered_df.shape)\n",
    "\n",
    "dataset_stats = dataset_stats_generator(filtered_df, draw_zero_distribution=True)\n",
    "print(\"Average Library Size: \", dataset_stats[0])\n",
    "print(\"Fraction of Zeroes: \", dataset_stats[1])\n",
    "print(\"Average Mean Expression: \", dataset_stats[2])\n",
    "print(\"Average Stdev Expression: \", dataset_stats[3])\n",
    "print(dataset_stats)\n",
    "\n",
    "\n",
    "\n",
    "new_tumours_counts = filtered_df.copy()\n",
    "new_tumours_counts['Mean'] = filtered_df.mean(axis=1)\n",
    "new_tumours_counts['StdDev'] = filtered_df.std(axis=1)\n",
    "new_tumours_counts['Min'] = filtered_df.min(axis=1)\n",
    "new_tumours_counts['Max'] = filtered_df.max(axis=1)\n",
    "\n",
    "top_10_means = new_tumours_counts.nlargest(10, 'Mean')\n",
    "\n",
    "# Step 4: Print the standard deviation of the filtered rows\n",
    "print(\"\\nTop 10 expressed genes\")\n",
    "print(top_10_means[['Mean', 'StdDev', 'Min', 'Max']])\n",
    "\n",
    "\n",
    "GSE181466_stdev = [] \n",
    "new_tumours_counts = filtered_df.copy()\n",
    "for i in range(min(10, len(new_tumours_counts))):\n",
    "    # Calculate the mean for each row\n",
    "    row_means = new_tumours_counts.mean(axis=1)\n",
    "    \n",
    "    # Calculate the standard deviation of these means\n",
    "    std_dev_of_means = row_means.std(ddof=0)\n",
    "    \n",
    "    # Append the standard deviation to the list\n",
    "    GSE181466_stdev.append(std_dev_of_means)\n",
    "    \n",
    "    # Remove the row with the highest mean\n",
    "    max_mean_index = row_means.idxmax()\n",
    "    new_tumours_counts.drop(index=max_mean_index, inplace=True)\n",
    "\n",
    "# print(dataset_stats)\n",
    "\n",
    "print(GSE181466_stdev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8701f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the trimming percentage\n",
    "trim_percent = 0.015  # 1% trimming\n",
    "\n",
    "# Function to calculate trimmed mean for a row\n",
    "def trimmed_mean(row, proportion):\n",
    "    sorted_row = sorted(row)\n",
    "    n = len(sorted_row)\n",
    "    trim_count = int(n * proportion)\n",
    "    trimmed_row = sorted_row[trim_count:n-trim_count]\n",
    "    return sum(trimmed_row) / len(trimmed_row) if trimmed_row else float('nan')\n",
    "\n",
    "# Apply the trimmed mean function to each row\n",
    "trimmed_means = filtered_df.apply(lambda row: trimmed_mean(row, trim_percent), axis=1)\n",
    "\n",
    "# Calculate the mean of the trimmed means\n",
    "overall_mean = trimmed_means.mean()\n",
    "\n",
    "#print(\"Trimmed Means for Each Row:\")\n",
    "#print(trimmed_means)\n",
    "#print(\"\\nOverall Mean of Trimmed Means:\")\n",
    "#print(overall_mean)\n",
    "\n",
    "search_term = 'NEAT'\n",
    "\n",
    "\n",
    "matching_rows = [name for name in trimmed_means.index if search_term in name]\n",
    "\n",
    "print(\"Matching Row Names:\")\n",
    "for name in matching_rows:\n",
    "    print(trimmed_means.loc[name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933efedc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-03T20:20:33.986046Z",
     "iopub.status.busy": "2024-05-03T20:20:33.985866Z",
     "iopub.status.idle": "2024-05-03T20:20:33.990610Z",
     "shell.execute_reply": "2024-05-03T20:20:33.989947Z"
    }
   },
   "outputs": [],
   "source": [
    "if (calc_AIC_dist):\n",
    "    print(\"GSE181466 - How off NB is to the winning distribution\")\n",
    "    aic_off = filtered_df.apply(manual_aic_distfromNB, axis=1)\n",
    "    print(aic_off)\n",
    "\n",
    "    # I want the average, and I want to eliminate -9999 (that's if all genes are zeroes)\n",
    "    mask = (aic_off != 0) & (aic_off > -9999)\n",
    "\n",
    "    median_off = np.median(aic_off[mask])\n",
    "    print(\"NB AIC is usually: \", median_off)\n",
    "    count_less_than_10 = np.sum(aic_off[mask] < 0.01)\n",
    "    count_greater_equal_10 = np.sum(aic_off[mask] >= 0.01)\n",
    "\n",
    "    print(f\"Count of values < 1%: {count_less_than_10}\")\n",
    "    print(f\"Count of values >= 1: {count_greater_equal_10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477de0d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-03T20:20:33.994246Z",
     "iopub.status.busy": "2024-05-03T20:20:33.994088Z",
     "iopub.status.idle": "2024-05-03T20:27:19.476878Z",
     "shell.execute_reply": "2024-05-03T20:27:19.475917Z"
    }
   },
   "outputs": [],
   "source": [
    "#filtered_df = filtered_df.iloc[1:200]\n",
    "\n",
    "print(\"GSE181466\")\n",
    "\n",
    "aic_values = filtered_df.apply(manual_aic, axis=1)\n",
    "aic_values, nb_theta_GSE181466, zinb_theta_GSE181466, zinb_pi_GSE181466 = zip(*aic_values)\n",
    "AIC_top_rank_GSE181466 = pd.Series(aic_values).value_counts()\n",
    "\n",
    "print(AIC_top_rank_GSE181466)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa9cd8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-03T20:27:19.490142Z",
     "iopub.status.busy": "2024-05-03T20:27:19.489991Z",
     "iopub.status.idle": "2024-05-03T20:27:20.248003Z",
     "shell.execute_reply": "2024-05-03T20:27:20.247104Z"
    }
   },
   "outputs": [],
   "source": [
    "## here, we will repeat our plots but for a different data set\n",
    "all_counts = pyreadr.read_r('/path/to/third_party/GSE146889_third_party_ffpe/GSE146889_GeneCount.rds')\n",
    "df = all_counts[None] # load all_counts into a pandas data frame\n",
    "\n",
    "\n",
    "# we need to split the tumors and normals by name\n",
    "count_TUMOR = df.filter(like='tumor')\n",
    "count_NORMAL = df.filter(like='normal')\n",
    "\n",
    "#filtered_tumour = count_TUMOR[count_TUMOR.sum(axis=1) > 1]\n",
    "#filtered_normal = count_NORMAL[count_NORMAL.sum(axis=1) > 1]\n",
    "\n",
    "# adjust for library size (fraction method)\n",
    "count_TUMOR_libadjust = library_adjust(count_TUMOR)\n",
    "\n",
    "fraction_of_zeroes = (count_TUMOR_libadjust == 0).mean(axis=1)\n",
    "filtered_tumour = count_TUMOR_libadjust[fraction_of_zeroes < (1 - express_percent_limit)] # must be expressed to this percentage of patients\n",
    "\n",
    "print(\"Tumours (GSE146889)\")\n",
    "dataset_stats = dataset_stats_generator(filtered_tumour, draw_zero_distribution=True, dataset_name=\"GSE146889 Tumours\")\n",
    "print(\"Average Library Size: \", dataset_stats[0])\n",
    "print(\"Fraction of Zeroes: \", dataset_stats[1])\n",
    "print(\"Average Mean Expression: \", dataset_stats[2])\n",
    "print(\"Average Stdev Expression: \", dataset_stats[3])\n",
    "print(count_TUMOR_libadjust.shape)\n",
    "print(filtered_tumour.shape)\n",
    "\n",
    "\n",
    "new_tumours_counts = filtered_tumour.copy()\n",
    "new_tumours_counts['Mean'] = filtered_tumour.mean(axis=1)\n",
    "new_tumours_counts['StdDev'] = filtered_tumour.std(axis=1)\n",
    "new_tumours_counts['Min'] = filtered_tumour.min(axis=1)\n",
    "new_tumours_counts['Max'] = filtered_tumour.max(axis=1)\n",
    "\n",
    "top_10_means = new_tumours_counts.nlargest(10, 'Mean')\n",
    "\n",
    "# Step 4: Print the standard deviation of the filtered rows\n",
    "print(\"\\nTop 10 expressed genes\")\n",
    "print(top_10_means[['Mean', 'StdDev', 'Min', 'Max']])\n",
    "\n",
    "\n",
    "GSE146889_tumour_stdev = [] \n",
    "new_tumours_counts = filtered_tumour.copy()\n",
    "for i in range(min(10, len(new_tumours_counts))):\n",
    "    # Calculate the mean for each row\n",
    "    row_means = new_tumours_counts.mean(axis=1)\n",
    "    \n",
    "    # Calculate the standard deviation of these means\n",
    "    std_dev_of_means = row_means.std(ddof=0)\n",
    "    \n",
    "    # Append the standard deviation to the list\n",
    "    GSE146889_tumour_stdev.append(std_dev_of_means)\n",
    "    \n",
    "    # Remove the row with the highest mean\n",
    "    max_mean_index = row_means.idxmax()\n",
    "    new_tumours_counts.drop(index=max_mean_index, inplace=True)\n",
    "\n",
    "# print(dataset_stats)\n",
    "\n",
    "print(GSE146889_tumour_stdev)\n",
    "\n",
    "\n",
    "#print(dataset_stats)\n",
    "# adjust for library size (fraction method)\n",
    "count_NORMAL_libadjust = library_adjust(count_NORMAL)\n",
    "\n",
    "fraction_of_zeroes = (count_NORMAL_libadjust == 0).mean(axis=1)\n",
    "filtered_normal = count_NORMAL_libadjust[fraction_of_zeroes < (1 - express_percent_limit)] # must be expressed to this percentage of patients\n",
    "\n",
    "print(\"Normals\")\n",
    "dataset_stats = dataset_stats_generator(filtered_normal, draw_zero_distribution=True, dataset_name=\"GSE146889 Normals\")\n",
    "print(\"Average Library Size: \", dataset_stats[0])\n",
    "print(\"Fraction of Zeroes: \", dataset_stats[1])\n",
    "print(\"Average Mean Expression: \", dataset_stats[2])\n",
    "print(\"Average Stdev Expression: \", dataset_stats[3])\n",
    "#print(dataset_stats)\n",
    "\n",
    "new_tumours_counts = filtered_normal.copy()\n",
    "new_tumours_counts['Mean'] = filtered_normal.mean(axis=1)\n",
    "new_tumours_counts['StdDev'] = filtered_normal.std(axis=1)\n",
    "new_tumours_counts['Min'] = filtered_normal.min(axis=1)\n",
    "new_tumours_counts['Max'] = filtered_normal.max(axis=1)\n",
    "\n",
    "top_10_means = new_tumours_counts.nlargest(10, 'Mean')\n",
    "\n",
    "# Step 4: Print the standard deviation of the filtered rows\n",
    "print(\"\\nTop 10 expressed genes\")\n",
    "print(top_10_means[['Mean', 'StdDev', 'Min', 'Max']])\n",
    "\n",
    "\n",
    "GSE146889_normal_stdev = [] \n",
    "new_tumours_counts = filtered_normal.copy()\n",
    "for i in range(min(10, len(new_tumours_counts))):\n",
    "    # Calculate the mean for each row\n",
    "    row_means = new_tumours_counts.mean(axis=1)\n",
    "    \n",
    "    # Calculate the standard deviation of these means\n",
    "    std_dev_of_means = row_means.std(ddof=0)\n",
    "    \n",
    "    # Append the standard deviation to the list\n",
    "    GSE146889_normal_stdev.append(std_dev_of_means)\n",
    "    \n",
    "    # Remove the row with the highest mean\n",
    "    max_mean_index = row_means.idxmax()\n",
    "    new_tumours_counts.drop(index=max_mean_index, inplace=True)\n",
    "\n",
    "# print(dataset_stats)\n",
    "\n",
    "print(GSE146889_normal_stdev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faad1075",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-03T20:27:20.252035Z",
     "iopub.status.busy": "2024-05-03T20:27:20.251857Z",
     "iopub.status.idle": "2024-05-03T20:38:42.641404Z",
     "shell.execute_reply": "2024-05-03T20:38:42.640553Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#filtered_tumour = filtered_tumour.iloc[1:200]\n",
    "\n",
    "print(\"GSE146889 - Tumours\")\n",
    "\n",
    "aic_values = filtered_tumour.apply(manual_aic, axis=1)\n",
    "aic_values, nb_theta_GSE146889_tumours, zinb_theta_GSE146889_tumours, zinb_pi_GSE146889_tumours = zip(*aic_values)\n",
    "AIC_top_rank_GSE146889_tumours = pd.Series(aic_values).value_counts()\n",
    "print(AIC_top_rank_GSE146889_tumours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d578082",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-03T20:38:42.645988Z",
     "iopub.status.busy": "2024-05-03T20:38:42.645821Z",
     "iopub.status.idle": "2024-05-03T20:49:21.214473Z",
     "shell.execute_reply": "2024-05-03T20:49:21.213596Z"
    }
   },
   "outputs": [],
   "source": [
    "#filtered_normal = filtered_normal.iloc[1:200]\n",
    "\n",
    "print(\"GSE146889 - Normal\")\n",
    "\n",
    "aic_values = filtered_normal.apply(manual_aic, axis=1)\n",
    "\n",
    "aic_values, nb_theta_GSE146889_normals, zinb_theta_GSE146889_normals, zinb_pi_GSE146889_normals = zip(*aic_values)\n",
    "\n",
    "AIC_top_rank_GSE146889_normals = pd.Series(aic_values).value_counts()\n",
    "print(AIC_top_rank_GSE146889_normals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd98a72f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-03T20:49:21.223287Z",
     "iopub.status.busy": "2024-05-03T20:49:21.223152Z",
     "iopub.status.idle": "2024-05-03T20:49:22.093505Z",
     "shell.execute_reply": "2024-05-03T20:49:22.092798Z"
    }
   },
   "outputs": [],
   "source": [
    "all_counts = pyreadr.read_r('/path/to/third_party/GSE209998_third_party_ffpe/GSE209998_GeneCount.rds')\n",
    "sample_information = pyreadr.read_r('/path/to/third_party/GSE209998_third_party_ffpe/GSE209998_Sample_Data.rds')\n",
    "\n",
    "# now we want to isolate just the expression from a particular type of tissue\n",
    "df_counts = all_counts[None] # load all_counts into a pandas data frame\n",
    "df_sample = sample_information[None] # load all_counts into a pandas data frame\n",
    "\n",
    "# here, we need to match if a sample is normal or tumour by !Sample_source_name_ch1 row\n",
    "\n",
    "# so I need to: 1) match columns between sample_information and all_counts \n",
    "# are they in the same order\n",
    "columns_df1 = df_counts.columns\n",
    "columns_df2 = df_sample.columns\n",
    "\n",
    "# Now we find what samples were tumours and what were normal\n",
    "samples_row = df_sample.loc[\"!Sample_source_name_ch1\"]\n",
    "\n",
    "split_dfs = {}\n",
    "for sample_type in samples_row.unique():\n",
    "    matching_columns = [col for col in df_counts.columns if col in df_sample.columns and samples_row[col] == sample_type]\n",
    "    split_dfs[sample_type] = df_counts[matching_columns]\n",
    "\n",
    "sample_source = df_sample.loc[\"!Sample_source\"]\n",
    "\n",
    "split_source = {}\n",
    "for sample_type in sample_source.unique():\n",
    "    matching_columns = [col for col in df_counts.columns if col in df_sample.columns and sample_source[col] == sample_type]\n",
    "    split_source[sample_type] = df_counts[matching_columns]\n",
    "\n",
    "\n",
    "count_FRESH = split_source[\"Fresh frozen\"]\n",
    "count_FFPE = split_source[\"FFPE\"]\n",
    "\n",
    "#filtered_ffpe = count_FFPE[count_FFPE.sum(axis=1) > 1]\n",
    "# adjust for library size (fraction method)\n",
    "count_FFPE_libadjust = library_adjust(count_FFPE)\n",
    "fraction_of_zeroes = (count_FFPE_libadjust == 0).mean(axis=1)\n",
    "filtered_ffpe = np.round(count_FFPE_libadjust[fraction_of_zeroes < (1 - express_percent_limit)]) # must be expressed to this percentage of patients\n",
    "\n",
    "#print(np.average(filtered_ffpe[0:1]), np.std(filtered_ffpe[0:1]))\n",
    "\n",
    "print(\"FFPE\")\n",
    "dataset_stats = dataset_stats_generator(filtered_ffpe, draw_zero_distribution=True, dataset_name = \"GSE209998 FFPE\")\n",
    "print(\"Average Library Size: \", dataset_stats[0])\n",
    "print(\"Fraction of Zeroes: \", dataset_stats[1])\n",
    "print(\"Average Mean Expression: \", dataset_stats[2])\n",
    "print(\"Average Stdev Expression: \", dataset_stats[3])\n",
    "# print(dataset_stats)\n",
    "\n",
    "print(\"FFPE\", filtered_ffpe.shape)\n",
    "\n",
    "\n",
    "new_tumours_counts = filtered_ffpe.copy()\n",
    "new_tumours_counts['Mean'] = filtered_ffpe.mean(axis=1)\n",
    "new_tumours_counts['StdDev'] = filtered_ffpe.std(axis=1)\n",
    "new_tumours_counts['Min'] = filtered_ffpe.min(axis=1)\n",
    "new_tumours_counts['Max'] = filtered_ffpe.max(axis=1)\n",
    "\n",
    "top_10_means = new_tumours_counts.nlargest(10, 'Mean')\n",
    "\n",
    "# Step 4: Print the standard deviation of the filtered rows\n",
    "print(\"\\nTop 10 expressed genes\")\n",
    "print(top_10_means[['Mean', 'StdDev', 'Min', 'Max']])\n",
    "\n",
    "\n",
    "GSE209998_ffpe_stdev = [] \n",
    "new_tumours_counts = filtered_ffpe.copy()\n",
    "for i in range(min(10, len(new_tumours_counts))):\n",
    "    # Calculate the mean for each row\n",
    "    row_means = new_tumours_counts.mean(axis=1)\n",
    "    \n",
    "    # Calculate the standard deviation of these means\n",
    "    std_dev_of_means = row_means.std(ddof=0)\n",
    "    \n",
    "    # Append the standard deviation to the list\n",
    "    GSE209998_ffpe_stdev.append(std_dev_of_means)\n",
    "    \n",
    "    # Remove the row with the highest mean\n",
    "    max_mean_index = row_means.idxmax()\n",
    "    new_tumours_counts.drop(index=max_mean_index, inplace=True)\n",
    "\n",
    "# print(dataset_stats)\n",
    "\n",
    "print(GSE209998_ffpe_stdev)\n",
    "\n",
    "\n",
    "# adjust for library size (fraction method)\n",
    "count_FRESH_libadjust = library_adjust(count_FRESH)\n",
    "fraction_of_zeroes = (count_FRESH_libadjust == 0).mean(axis=1)\n",
    "filtered_fresh = np.round(count_FRESH_libadjust[fraction_of_zeroes < (1 - express_percent_limit)]) # must be expressed to this percentage of patients\n",
    "print(\"Fresh\", filtered_fresh.shape[1])\n",
    "\n",
    "dataset_stats = dataset_stats_generator(filtered_fresh, draw_zero_distribution=True, dataset_name = \"GSE209998 Formalin-Fixed\")\n",
    "print(\"Average Library Size: \", dataset_stats[0])\n",
    "print(\"Fraction of Zeroes: \", dataset_stats[1])\n",
    "print(\"Average Mean Expression: \", dataset_stats[2])\n",
    "print(\"Average Stdev Expression: \", dataset_stats[3])\n",
    "\n",
    "\n",
    "\n",
    "new_tumours_counts = filtered_fresh.copy()\n",
    "new_tumours_counts['Mean'] = filtered_fresh.mean(axis=1)\n",
    "new_tumours_counts['StdDev'] = filtered_fresh.std(axis=1)\n",
    "new_tumours_counts['Min'] = filtered_fresh.min(axis=1)\n",
    "new_tumours_counts['Max'] = filtered_fresh.max(axis=1)\n",
    "\n",
    "top_10_means = new_tumours_counts.nlargest(10, 'Mean')\n",
    "\n",
    "# Step 4: Print the standard deviation of the filtered rows\n",
    "print(\"\\nTop 10 expressed genes\")\n",
    "print(top_10_means[['Mean', 'StdDev', 'Min', 'Max']])\n",
    "\n",
    "\n",
    "GSE209998_ff_stdev = [] \n",
    "new_tumours_counts = filtered_fresh.copy()\n",
    "for i in range(min(10, len(new_tumours_counts))):\n",
    "    # Calculate the mean for each row\n",
    "    row_means = new_tumours_counts.mean(axis=1)\n",
    "    \n",
    "    # Calculate the standard deviation of these means\n",
    "    std_dev_of_means = row_means.std(ddof=0)\n",
    "    \n",
    "    # Append the standard deviation to the list\n",
    "    GSE209998_ff_stdev.append(std_dev_of_means)\n",
    "    \n",
    "    # Remove the row with the highest mean\n",
    "    max_mean_index = row_means.idxmax()\n",
    "    new_tumours_counts.drop(index=max_mean_index, inplace=True)\n",
    "\n",
    "# print(dataset_stats)\n",
    "\n",
    "print(GSE209998_ff_stdev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dd55aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the trimming percentage\n",
    "trim_percent = 0.00  # 1% trimming is 0.01\n",
    "\n",
    "# Function to calculate trimmed mean for a row\n",
    "def right_tailed_trimmed_mean(row, proportion):\n",
    "    sorted_row = sorted(row)\n",
    "    n = len(sorted_row)\n",
    "    trim_count = int(n * proportion)\n",
    "    # Trim only from the right\n",
    "    trimmed_row = sorted_row[:n-trim_count]\n",
    "    return sum(trimmed_row) / len(trimmed_row) if trimmed_row else float('nan')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Apply the trimmed mean function to each row\n",
    "trimmed_means = filtered_ffpe.apply(lambda row: right_tailed_trimmed_mean(row, trim_percent), axis=1)\n",
    "\n",
    "min_data = np.min(trimmed_means + 0.0000001)\n",
    "max_data = np.max(trimmed_means + 1)\n",
    "print(min_data, max_data)\n",
    "\n",
    "# Generate log-spaced bins\n",
    "bins = np.logspace(np.log10(min_data), np.log10(max_data), 500)\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.hist(trimmed_means, bins=bins, color='blue', alpha=0.7, log=True)\n",
    "plt.xlabel('Mean of Transcript Counts')\n",
    "plt.ylabel('Counts (log-scaled)')\n",
    "#plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "#plt.xlim(left=1)\n",
    "dataset_name = \"GSE209998 FFPE\"\n",
    "plt.title(\"Distribution of Mean Counts Per Transcript: \" + dataset_name, size=8)\n",
    "\n",
    "outpath = \"/path/to/6.0.1_Third_Party_Data.Best_AIC_and_Stat_Generator.No_GLM/\"\n",
    "filename = 'GSE209998_FFPE_Log_Distribution.pdf'\n",
    "plt.savefig(outpath + filename, format='pdf', dpi=300, bbox_inches='tight') \n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Calculate the mean of the trimmed means\n",
    "overall_mean = trimmed_means.mean()\n",
    "\n",
    "#print(\"Trimmed Means for Each Row:\")\n",
    "#print(trimmed_means)\n",
    "#print(\"\\nOverall Mean of Trimmed Means:\")\n",
    "#print(overall_mean)\n",
    "\n",
    "search_term = 'MALAT1'\n",
    "\n",
    "matching_rows = [name for name in trimmed_means.index if search_term in name]\n",
    "\n",
    "print(\"Matching Row Names:\")\n",
    "for name in matching_rows:\n",
    "    print(trimmed_means.loc[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654d928f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try out making a plot. Log mean vs log variance.\n",
    "\n",
    "trim_percent = 0.00  # 1% trimming is 0.01\n",
    "\n",
    "def right_tailed_trimmed_mean_and_variance(row, proportion):\n",
    "    sorted_row = sorted(row)\n",
    "    n = len(sorted_row)\n",
    "    trim_count = int(n * proportion)\n",
    "    # Trim only from the right (highest values)\n",
    "    trimmed_row = sorted_row[:n - trim_count]\n",
    "    if not trimmed_row:\n",
    "        return float('nan'), float('nan')  # Return NaN if trimmed_row is empty\n",
    "\n",
    "    # Calculate the trimmed mean\n",
    "    mean = sum(trimmed_row) / len(trimmed_row)\n",
    "    \n",
    "    # Calculate the trimmed variance\n",
    "    if len(trimmed_row) > 1:\n",
    "        variance = sum((x - mean) ** 2 for x in trimmed_row) / (len(trimmed_row) - 1)\n",
    "    else:\n",
    "        variance = 0.0  # Variance is zero if only one data point remains\n",
    "\n",
    "    return mean, variance\n",
    "\n",
    "\n",
    "trimmed_means = []\n",
    "trimmed_variances = []\n",
    "for index, row in filtered_ffpe.iterrows():\n",
    "    mean, variance = right_tailed_trimmed_mean_and_variance(row, trim_percent)\n",
    "    # Only include valid results\n",
    "    if not np.isnan(mean) and mean > 0 and variance > 0:\n",
    "        trimmed_means.append(mean)\n",
    "        trimmed_variances.append(variance)\n",
    "\n",
    "\n",
    "epsilon = 1e-10\n",
    "log_trimmed_means = np.log(np.array(trimmed_means) + epsilon)\n",
    "log_trimmed_variances = np.log(np.array(trimmed_variances) + epsilon)\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.scatter(log_trimmed_means, log_trimmed_variances, alpha=0.6, edgecolors='w', s=50)\n",
    "\n",
    "# Plot the green line where variance equals mean (slope = 1)\n",
    "x_values = np.linspace(np.min(log_trimmed_means), np.max(log_trimmed_means), 100)\n",
    "y_values_poisson = x_values  # Since log(var) = log(mean)\n",
    "\n",
    "plt.plot(x_values, y_values_poisson, color='green', linestyle='--', label='Variance = Mean (Poisson)')\n",
    "\n",
    "# Plot the red line where variance equals mean squared (slope = 2)\n",
    "y_values_quadratic = 2 * x_values  # Since log(var) = 2 * log(mean)\n",
    "\n",
    "plt.plot(x_values, y_values_quadratic, color='red', linestyle='--', label='Variance = Mean')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Log (Mean)')\n",
    "plt.ylabel('Log (Variance)')\n",
    "dataset_name = \"GSE209998 FFPE\"\n",
    "plt.title('Log(Mean) vs. Log(Variance): ' + dataset_name)\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "\n",
    "# Add grid\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "hb = plt.hexbin(log_trimmed_means, log_trimmed_variances, gridsize=50, cmap='viridis', mincnt=1)\n",
    "\n",
    "# Add a colorbar\n",
    "cb = plt.colorbar(hb)\n",
    "cb.set_label('Counts')\n",
    "\n",
    "# Plot the theoretical lines\n",
    "x_values = np.linspace(np.min(log_trimmed_means), np.max(log_trimmed_means), 100)\n",
    "\n",
    "# Variance equals mean (Poisson)\n",
    "y_values_poisson = x_values\n",
    "plt.plot(x_values, y_values_poisson, color='green', linestyle='--', label='Variance = Mean (Poisson)')\n",
    "\n",
    "# Variance equals mean squared\n",
    "y_values_quadratic = 2 * x_values\n",
    "plt.plot(x_values, y_values_quadratic, color='red', linestyle='--', label='Variance = Mean')\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel('Log (Mean)')\n",
    "plt.ylabel('Log (Variance)')\n",
    "plt.title('Log(Mean) vs. Log(Variance): ' + dataset_name, size=8)\n",
    "\n",
    "# Legend and grid\n",
    "plt.legend(fontsize=8)\n",
    "plt.grid(True)\n",
    "\n",
    "outpath = \"/path/to/6.0.1_Third_Party_Data.Best_AIC_and_Stat_Generator.No_GLM/\"\n",
    "filename = 'GSE209998_FFPE_Log_Mean_vs_Log_Var.pdf'\n",
    "plt.savefig(outpath + filename, format='pdf', dpi=300, bbox_inches='tight') \n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeda9dcf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c73f758",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-03T20:49:22.095951Z",
     "iopub.status.busy": "2024-05-03T20:49:22.095787Z",
     "iopub.status.idle": "2024-05-03T21:05:55.625014Z",
     "shell.execute_reply": "2024-05-03T21:05:55.624260Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#filtered_ffpe = filtered_ffpe.iloc[1:200]\n",
    "\n",
    "print(\"GSE209998 - FFPE Tumours\")\n",
    "\n",
    "aic_values = filtered_ffpe.apply(manual_aic, axis=1)\n",
    "\n",
    "aic_values, nb_theta_GSE209998_ffpe, zinb_theta_GSE209998_ffpe, zinb_pi_GSE209998_ffpe = zip(*aic_values)\n",
    "\n",
    "AIC_top_rank_GSE209998_ffpe = pd.Series(aic_values).value_counts()\n",
    "\n",
    "print(AIC_top_rank_GSE209998_ffpe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579cb9e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-03T21:05:55.636540Z",
     "iopub.status.busy": "2024-05-03T21:05:55.636355Z",
     "iopub.status.idle": "2024-05-03T21:22:34.967532Z",
     "shell.execute_reply": "2024-05-03T21:22:34.966821Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"GSE209998 - Fresh/Frozen Tumours\")\n",
    "\n",
    "aic_values = filtered_fresh.apply(manual_aic, axis=1)\n",
    "aic_values, nb_theta_GSE209998_fresh, zinb_theta_GSE209998_fresh, zinb_pi_GSE209998_fresh = zip(*aic_values)\n",
    "\n",
    "AIC_top_rank_GSE209998_fresh = pd.Series(aic_values).value_counts() \n",
    "\n",
    "print(AIC_top_rank_GSE209998_fresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea06eee5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35a7770",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcb4399",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-03T21:22:34.972370Z",
     "iopub.status.busy": "2024-05-03T21:22:34.972208Z",
     "iopub.status.idle": "2024-05-03T21:22:35.079083Z",
     "shell.execute_reply": "2024-05-03T21:22:35.078378Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('/path/to/third_party/GSE47462_third_party_ffpe/GSE47462_Raw_counts_Refseq_genes.txt',\n",
    "                  delimiter='\\t')\n",
    "# Split the DataFrame into subsets based on column names indicating sample type\n",
    "normal_data = data.filter(like='_normal')\n",
    "EN_data = data.filter(like='_EN')\n",
    "DCIS_data = data.filter(like='_DCIS')\n",
    "IDC_data = data.filter(like='_IDC')\n",
    "\n",
    "# since there isn't a ton of data, I also want to group tumors\n",
    "tumours_data = data.loc[:, ~data.columns.str.contains('_normal')]\n",
    "#print(tumours_data)\n",
    "tumours_data.set_index('symbol', inplace=True)\n",
    "\n",
    "normal_data.index = tumours_data.index\n",
    "\n",
    "#tumours_data = tumours_data.iloc[:, 1:]\n",
    "\n",
    "#filtered_tumour = tumours_counts[tumours_counts.sum(axis=1) > 1]\n",
    "#filtered_normal = normal_counts[normal_counts.sum(axis=1) > 1]\n",
    "\n",
    "# adjust for library size (fraction method)\n",
    "tumours_data_libadjust = library_adjust(tumours_data)\n",
    "fraction_of_zeroes = (tumours_data_libadjust == 0).mean(axis=1)\n",
    "filtered_tumour = tumours_data_libadjust[fraction_of_zeroes < (1 - express_percent_limit)] # must be expressed to this percentage of patients\n",
    "\n",
    "normal_data_libadjust = library_adjust(normal_data)\n",
    "fraction_of_zeroes = (normal_data_libadjust == 0).mean(axis=1)\n",
    "filtered_normal = normal_data_libadjust[fraction_of_zeroes < (1 - express_percent_limit)] # must be expressed to this percentage of patients\n",
    "\n",
    "print(filtered_tumour.shape)\n",
    "print(filtered_normal.shape)\n",
    "\n",
    "print(\"Tumour\")\n",
    "dataset_stats = dataset_stats_generator(filtered_tumour, draw_zero_distribution=True, dataset_name=\"GSE47462 Tumours\")\n",
    "print(\"Average Library Size: \", dataset_stats[0])\n",
    "print(\"Fraction of Zeroes: \", dataset_stats[1])\n",
    "print(\"Average Mean Expression: \", dataset_stats[2])\n",
    "print(\"Average Stdev Expression: \", dataset_stats[3])\n",
    "# print(dataset_stats)\n",
    "\n",
    "new_tumours_counts = filtered_tumour.copy()\n",
    "new_tumours_counts['Mean'] = filtered_tumour.mean(axis=1)\n",
    "new_tumours_counts['StdDev'] = filtered_tumour.std(axis=1)\n",
    "new_tumours_counts['Median'] = filtered_tumour.median(axis=1)\n",
    "new_tumours_counts['Min'] = filtered_tumour.min(axis=1)\n",
    "new_tumours_counts['Max'] = filtered_tumour.max(axis=1)\n",
    "\n",
    "top_10_means = new_tumours_counts.nlargest(10, 'Mean')\n",
    "\n",
    "# Step 4: Print the standard deviation of the filtered rows\n",
    "print(\"\\nTop 10 expressed genes\")\n",
    "print(top_10_means[['Mean', 'StdDev', 'Median', 'Min', 'Max']])\n",
    "\n",
    "\n",
    "GSE47462_tumour_stdev = [] \n",
    "new_tumours_counts = filtered_tumour.copy()\n",
    "for i in range(min(10, len(new_tumours_counts))):\n",
    "    # Calculate the mean for each row\n",
    "    row_means = new_tumours_counts.mean(axis=1)\n",
    "    \n",
    "    # Calculate the standard deviation of these means\n",
    "    std_dev_of_means = row_means.std(ddof=0)\n",
    "    \n",
    "    # Append the standard deviation to the list\n",
    "    GSE47462_tumour_stdev.append(std_dev_of_means)\n",
    "    \n",
    "    # Remove the row with the highest mean\n",
    "    max_mean_index = row_means.idxmax()\n",
    "    new_tumours_counts.drop(index=max_mean_index, inplace=True)\n",
    "\n",
    "# print(dataset_stats)\n",
    "\n",
    "print(GSE47462_tumour_stdev)\n",
    "\n",
    "print(\"Normal\")\n",
    "dataset_stats = dataset_stats_generator(filtered_normal, draw_zero_distribution=True, dataset_name=\"GSE47462 Normals\")\n",
    "print(\"Average Library Size: \", dataset_stats[0])\n",
    "print(\"Fraction of Zeroes: \", dataset_stats[1])\n",
    "print(\"Average Mean Expression: \", dataset_stats[2])\n",
    "print(\"Average Stdev Expression: \", dataset_stats[3])\n",
    "# print(dataset_stats)\n",
    "\n",
    "new_tumours_counts = filtered_normal.copy()\n",
    "new_tumours_counts['Mean'] = filtered_normal.mean(axis=1)\n",
    "new_tumours_counts['StdDev'] = filtered_normal.std(axis=1)\n",
    "new_tumours_counts['Min'] = filtered_normal.min(axis=1)\n",
    "new_tumours_counts['Max'] = filtered_normal.max(axis=1)\n",
    "\n",
    "top_10_means = new_tumours_counts.nlargest(10, 'Mean')\n",
    "\n",
    "# Step 4: Print the standard deviation of the filtered rows\n",
    "print(\"\\nTop 10 expressed genes\")\n",
    "print(top_10_means[['Mean', 'StdDev', 'Min', 'Max']])\n",
    "\n",
    "\n",
    "GSE47462_normal_stdev = [] \n",
    "new_tumours_counts = filtered_normal.copy()\n",
    "for i in range(min(10, len(new_tumours_counts))):\n",
    "    # Calculate the mean for each row\n",
    "    row_means = new_tumours_counts.mean(axis=1)\n",
    "    \n",
    "    # Calculate the standard deviation of these means\n",
    "    std_dev_of_means = row_means.std(ddof=0)\n",
    "    \n",
    "    # Append the standard deviation to the list\n",
    "    GSE47462_normal_stdev.append(std_dev_of_means)\n",
    "    \n",
    "    # Remove the row with the highest mean\n",
    "    max_mean_index = row_means.idxmax()\n",
    "    new_tumours_counts.drop(index=max_mean_index, inplace=True)\n",
    "\n",
    "# print(dataset_stats)\n",
    "\n",
    "print(GSE47462_normal_stdev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19e141e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-03T21:22:35.082765Z",
     "iopub.status.busy": "2024-05-03T21:22:35.082615Z",
     "iopub.status.idle": "2024-05-03T21:29:43.291515Z",
     "shell.execute_reply": "2024-05-03T21:29:43.290565Z"
    }
   },
   "outputs": [],
   "source": [
    "#filtered_tumour = filtered_tumour.iloc[1:200]\n",
    "\n",
    "print(\"GSE47462 - Tumours\")\n",
    "\n",
    "aic_values = filtered_tumour.apply(manual_aic, axis=1)\n",
    "aic_values, nb_theta_GSE47462_tumours, zinb_theta_GSE47462_tumours, zinb_pi_GSE47462_tumours = zip(*aic_values)\n",
    "\n",
    "AIC_top_rank_GSE47462_tumours = pd.Series(aic_values).value_counts() \n",
    "\n",
    "print(AIC_top_rank_GSE47462_tumours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb29ed55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-03T21:29:43.296370Z",
     "iopub.status.busy": "2024-05-03T21:29:43.296176Z",
     "iopub.status.idle": "2024-05-03T21:36:48.669912Z",
     "shell.execute_reply": "2024-05-03T21:36:48.669155Z"
    }
   },
   "outputs": [],
   "source": [
    "#filtered_normal = filtered_normal.iloc[1:200]\n",
    "\n",
    "print(\"GSE47462 - Normal\")\n",
    "\n",
    "aic_values = filtered_normal.apply(manual_aic, axis=1)\n",
    "aic_values, nb_theta_GSE47462_normals, zinb_theta_GSE47462_normals, zinb_pi_GSE47462_normals = zip(*aic_values)\n",
    "\n",
    "AIC_top_rank_GSE47462_normals = pd.Series(aic_values).value_counts()    \n",
    "\n",
    "print(AIC_top_rank_GSE47462_normals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c737f7bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673e2662",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-03T21:36:48.675220Z",
     "iopub.status.busy": "2024-05-03T21:36:48.675040Z",
     "iopub.status.idle": "2024-05-03T21:36:49.138674Z",
     "shell.execute_reply": "2024-05-03T21:36:49.137968Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "data = pd.read_csv('/path/to/third_party/GSE120795_third_party_ffpe/GSE120795_total_norms_raw_counts.tsv',\n",
    "                  delimiter='\\t')\n",
    "\n",
    "# in the series matrix\"disease: healthy\", \n",
    "patient_info = pd.read_csv('/path/to/third_party/GSE120795_third_party_ffpe/GSE120795_cell_info.txt',\n",
    "                  delimiter='\\t')\n",
    "\n",
    "# this filter is present because those filtered out were not FFPE (blood and bone marrow)\n",
    "mask = patient_info.iloc[0] == \"healthy\"\n",
    "\n",
    "filtered_data = patient_info.loc[:, mask]\n",
    "patient_names = filtered_data.columns\n",
    "column_names_with_extension = [name + \".fastq.gz\" for name in patient_names]\n",
    "column_names_with_extension = column_names_with_extension[1:]\n",
    "\n",
    "# Assuming 'second_list' is the list where you want to filter based on column names\n",
    "filtered_data = data[column_names_with_extension]\n",
    "ffpe_counts = pd.DataFrame(filtered_data)\n",
    "#filtered_data = ffpe_counts[ffpe_counts.sum(axis=1) > 1]\n",
    "print(ffpe_counts.shape)\n",
    "\n",
    "\n",
    "ffpe_counts_libadjust = library_adjust(ffpe_counts)\n",
    "fraction_of_zeroes = (ffpe_counts_libadjust == 0).mean(axis=1)\n",
    "filtered_data = ffpe_counts_libadjust[fraction_of_zeroes < (1 - express_percent_limit)] # must be expressed to this percentage of patients\n",
    "print(filtered_data.shape)\n",
    "\n",
    "dataset_stats = dataset_stats_generator(filtered_data, draw_zero_distribution=True, dataset_name=\"GSE120795\")\n",
    "print(\"Average Library Size: \", dataset_stats[0])\n",
    "print(\"Fraction of Zeroes: \", dataset_stats[1])\n",
    "print(\"Average Mean Expression: \", dataset_stats[2])\n",
    "print(\"Average Stdev Expression: \", dataset_stats[3])\n",
    "print(dataset_stats)\n",
    "\n",
    "\n",
    "new_tumours_counts = filtered_data.copy()\n",
    "new_tumours_counts['Mean'] = filtered_data.mean(axis=1)\n",
    "new_tumours_counts['StdDev'] = filtered_data.std(axis=1)\n",
    "new_tumours_counts['Median'] = filtered_data.max(axis=1)\n",
    "new_tumours_counts['Min'] = filtered_data.min(axis=1)\n",
    "new_tumours_counts['Max'] = filtered_data.max(axis=1)\n",
    "\n",
    "top_10_means = new_tumours_counts.nlargest(10, 'Mean')\n",
    "\n",
    "# Step 4: Print the standard deviation of the filtered rows\n",
    "print(\"\\nTop 10 expressed genes\")\n",
    "print(top_10_means[['Mean', 'StdDev', 'Median', 'Min', 'Max']])\n",
    "\n",
    "\n",
    "GSE120795_stdev = [] \n",
    "new_tumours_counts = filtered_data.copy()\n",
    "for i in range(min(10, len(new_tumours_counts))):\n",
    "    # Calculate the mean for each row\n",
    "    row_means = new_tumours_counts.mean(axis=1)\n",
    "    \n",
    "    # Calculate the standard deviation of these means\n",
    "    std_dev_of_means = row_means.std(ddof=0)\n",
    "    \n",
    "    # Append the standard deviation to the list\n",
    "    GSE120795_stdev.append(std_dev_of_means)\n",
    "    \n",
    "    # Remove the row with the highest mean\n",
    "    max_mean_index = row_means.idxmax()\n",
    "    new_tumours_counts.drop(index=max_mean_index, inplace=True)\n",
    "\n",
    "# print(dataset_stats)\n",
    "\n",
    "print(GSE120795_stdev)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b47a99a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-03T21:36:49.142886Z",
     "iopub.status.busy": "2024-05-03T21:36:49.142730Z",
     "iopub.status.idle": "2024-05-03T21:48:24.458671Z",
     "shell.execute_reply": "2024-05-03T21:48:24.457814Z"
    }
   },
   "outputs": [],
   "source": [
    "#filtered_data = filtered_data.iloc[1:200]\n",
    "\n",
    "print(\"GSE120795 - Normal\")\n",
    "\n",
    "aic_values = filtered_data.apply(manual_aic, axis=1)\n",
    "aic_values, nb_theta_GSE120795_normals, zinb_theta_GSE120795_normals, zinb_pi_GSE120795_normals = zip(*aic_values)\n",
    "\n",
    "AIC_top_rank_GSE120795_normals = pd.Series(aic_values).value_counts()  \n",
    "\n",
    "print(AIC_top_rank_GSE120795_normals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a213ed5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-03T21:48:24.471746Z",
     "iopub.status.busy": "2024-05-03T21:48:24.471587Z",
     "iopub.status.idle": "2024-05-03T21:48:25.038989Z",
     "shell.execute_reply": "2024-05-03T21:48:25.038170Z"
    }
   },
   "outputs": [],
   "source": [
    "# the GDC Count-Me-In Data\n",
    "data = pd.read_csv('/path/to/third_party/CountMeIn_BConly_third_party_ffpe/MBC_CMI_Compiled_Counts.tsv',\n",
    "                  delimiter=' ') # space delimited\n",
    "\n",
    "\n",
    "tumours_counts = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "tumours_counts = tumours_counts[tumours_counts['gene_type'].notna()]\n",
    "tumours_counts.set_index('gene_name', inplace=True)\n",
    "tumours_counts = tumours_counts.drop(tumours_counts.columns[:2], axis=1) # columns 1-3 should be ignored\n",
    "\n",
    "# library adjust; remove genes expressed < express_percent_limit\n",
    "tumours_counts_libadjust = library_adjust(tumours_counts)\n",
    "fraction_of_zeroes = (tumours_counts_libadjust == 0).mean(axis=1)\n",
    "filtered_df = tumours_counts_libadjust[fraction_of_zeroes < (1 - express_percent_limit)] # must be expressed to this percentage of patients\n",
    "print(filtered_df.shape)\n",
    "\n",
    "dataset_stats = dataset_stats_generator(filtered_df, draw_zero_distribution=True, dataset_name=\"TMBC Project\")\n",
    "print(\"Average Library Size: \", dataset_stats[0])\n",
    "print(\"Fraction of Zeroes: \", dataset_stats[1])\n",
    "print(\"Average Mean Expression: \", dataset_stats[2])\n",
    "print(\"Average Stdev Expression: \", dataset_stats[3])\n",
    "print(dataset_stats)\n",
    "\n",
    "\n",
    "\n",
    "new_tumours_counts = filtered_df.copy()\n",
    "new_tumours_counts['Mean'] = filtered_df.mean(axis=1)\n",
    "new_tumours_counts['StdDev'] = filtered_df.std(axis=1)\n",
    "new_tumours_counts['Min'] = filtered_df.min(axis=1)\n",
    "new_tumours_counts['Max'] = filtered_df.max(axis=1)\n",
    "\n",
    "top_10_means = new_tumours_counts.nlargest(10, 'Mean')\n",
    "\n",
    "# Step 4: Print the standard deviation of the filtered rows\n",
    "print(\"\\nTop 10 expressed genes\")\n",
    "print(top_10_means[['Mean', 'StdDev', 'Min', 'Max']])\n",
    "\n",
    "\n",
    "TMBC_stdev = [] \n",
    "new_tumours_counts = filtered_df.copy()\n",
    "for i in range(min(10, len(new_tumours_counts))):\n",
    "    # Calculate the mean for each row\n",
    "    row_means = new_tumours_counts.mean(axis=1)\n",
    "    \n",
    "    # Calculate the standard deviation of these means\n",
    "    std_dev_of_means = row_means.std(ddof=0)\n",
    "    \n",
    "    # Append the standard deviation to the list\n",
    "    TMBC_stdev.append(std_dev_of_means)\n",
    "    \n",
    "    # Remove the row with the highest mean\n",
    "    max_mean_index = row_means.idxmax()\n",
    "    new_tumours_counts.drop(index=max_mean_index, inplace=True)\n",
    "\n",
    "# print(dataset_stats)\n",
    "\n",
    "print(TMBC_stdev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058f2e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the trimming percentage\n",
    "trim_percent = 0.01  # 1% trimming\n",
    "\n",
    "# Function to calculate trimmed mean for a row\n",
    "def trimmed_mean(row, proportion):\n",
    "    sorted_row = sorted(row)\n",
    "    n = len(sorted_row)\n",
    "    trim_count = int(n * proportion)\n",
    "    trimmed_row = sorted_row[trim_count:n-trim_count]\n",
    "    return sum(trimmed_row) / len(trimmed_row) if trimmed_row else float('nan')\n",
    "\n",
    "# Apply the trimmed mean function to each row\n",
    "trimmed_means = filtered_df.apply(lambda row: trimmed_mean(row, trim_percent), axis=1)\n",
    "\n",
    "# Calculate the mean of the trimmed means\n",
    "overall_mean = trimmed_means.mean()\n",
    "\n",
    "#print(\"Trimmed Means for Each Row:\")\n",
    "#print(trimmed_means)\n",
    "#print(\"\\nOverall Mean of Trimmed Means:\")\n",
    "#print(overall_mean)\n",
    "\n",
    "search_term = 'XIST'\n",
    "\n",
    "\n",
    "matching_rows = [name for name in trimmed_means.index if search_term in name]\n",
    "\n",
    "print(\"Matching Row Names:\")\n",
    "for name in matching_rows:\n",
    "    print(trimmed_means.loc[name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb41814",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-03T21:48:25.043130Z",
     "iopub.status.busy": "2024-05-03T21:48:25.042959Z",
     "iopub.status.idle": "2024-05-03T21:59:37.058365Z",
     "shell.execute_reply": "2024-05-03T21:59:37.057337Z"
    }
   },
   "outputs": [],
   "source": [
    "#filtered_df = filtered_df.iloc[1:200]\n",
    "\n",
    "print(\"Count Me In - Breast Cancer Only\")\n",
    "\n",
    "aic_values = filtered_df.apply(manual_aic, axis=1)\n",
    "aic_values, nb_theta_CMI, zinb_theta_CMI, zinb_pi_CMI = zip(*aic_values)\n",
    "\n",
    "AIC_top_rank_CMI = pd.Series(aic_values).value_counts()  \n",
    "\n",
    "print(AIC_top_rank_CMI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b0bb39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-03T21:59:37.072021Z",
     "iopub.status.busy": "2024-05-03T21:59:37.071875Z",
     "iopub.status.idle": "2024-05-03T21:59:39.609859Z",
     "shell.execute_reply": "2024-05-03T21:59:39.609069Z"
    }
   },
   "outputs": [],
   "source": [
    "# Our dataset!\n",
    "all_counts = pyreadr.read_r('/path/to/dcis/expression_counts.Jan2023_1_2_and_2_2.rds')\n",
    "vst_norm = pyreadr.read_r('/path/to/dcis/expression_VST_Normalized.Jan2023_1_2_and_2_2.rds')\n",
    "\n",
    "# this data is loading without issue\n",
    "ship_data = pyreadr.read_r('/path/to/dcis/ship1_2_full_tbl.Jan2023.With_Stroma_Assignment.rds')\n",
    "# I wish that we could've simply used the RDA, but the counts-only RDS works and loads faster so what can you do\n",
    "# in the future, could try the package 'rpy2' instead, it's an alternative that requires R but that's okay for us\n",
    "\n",
    "# now we want to isolate just the expression from a particular type of tissue\n",
    "df = all_counts[None] # load all_counts into a pandas data frame\n",
    "\n",
    "# Eliminate any samples in the blacklist\n",
    "ship_df = ship_data[None]\n",
    "#print(ship_df['blacklist'].value_counts()) # they're all false\n",
    "\n",
    "# since ship_data already has patients filtered out, lets filter out any patient who isn't on the list\n",
    "# match by 'sample_name'\n",
    "df_blacklist_filtered = df[ship_df['sample_name']]\n",
    "\n",
    "# split the patients by tissue\n",
    "count_DCIS = df_blacklist_filtered.filter(like='_D')\n",
    "count_STROMA = df_blacklist_filtered.filter(like='_S')\n",
    "count_NORMAL = df_blacklist_filtered.filter(like='_N')\n",
    "\n",
    "# if we want consistency between the 3 sample types\n",
    "vst_table = vst_norm[None] # we don't apply this anymore because it blocks any gene with >80% frac_zero\n",
    "filtered_norm_count = count_NORMAL#[count_NORMAL.index.isin(vst_table.index)]\n",
    "filtered_tumour_count = count_DCIS#[count_DCIS.index.isin(vst_table.index)]\n",
    "filtered_stroma_count = count_STROMA#[count_STROMA.index.isin(vst_table.index)]\n",
    "\n",
    "filtered_norm_count_libadjust = library_adjust(filtered_norm_count)\n",
    "fraction_of_zeroes = (filtered_norm_count_libadjust == 0).mean(axis=1)\n",
    "filtered_norm_count = filtered_norm_count_libadjust[fraction_of_zeroes < (1 - express_percent_limit)] # must be expressed to this percentage of patients\n",
    "\n",
    "filtered_tumour_count_libadjust = library_adjust(filtered_tumour_count)\n",
    "fraction_of_zeroes = (filtered_tumour_count_libadjust == 0).mean(axis=1)\n",
    "filtered_tumour_count = filtered_tumour_count_libadjust[fraction_of_zeroes < (1 - express_percent_limit)] # must be expressed to this percentage of patients\n",
    "\n",
    "filtered_stroma_count_libadjust = library_adjust(filtered_stroma_count)\n",
    "fraction_of_zeroes = (filtered_stroma_count_libadjust == 0).mean(axis=1)\n",
    "filtered_stroma_count = filtered_stroma_count_libadjust[fraction_of_zeroes < (1 - express_percent_limit)] # must be expressed to this percentage of patients\n",
    "\n",
    "\n",
    "\n",
    "print(\"DCIS\")\n",
    "print(filtered_tumour_count.shape)\n",
    "dataset_stats = dataset_stats_generator(filtered_tumour_count, draw_zero_distribution=True, dataset_name=\"Sunnybrook DCIS\")\n",
    "print(\"Average Library Size: \", dataset_stats[0])\n",
    "print(\"Fraction of Zeroes: \", dataset_stats[1])\n",
    "print(\"Average Mean Expression: \", dataset_stats[2])\n",
    "print(\"Average Stdev Expression: \", dataset_stats[3])\n",
    "\n",
    "\n",
    "new_tumours_counts = filtered_tumour_count.copy()\n",
    "new_tumours_counts['Mean'] = filtered_tumour_count.mean(axis=1)\n",
    "new_tumours_counts['StdDev'] = filtered_tumour_count.std(axis=1)\n",
    "new_tumours_counts['Median'] = filtered_tumour_count.median(axis=1)\n",
    "new_tumours_counts['Min'] = filtered_tumour_count.min(axis=1)\n",
    "new_tumours_counts['Max'] = filtered_tumour_count.max(axis=1)\n",
    "\n",
    "top_10_means = new_tumours_counts.nlargest(10, 'Mean')\n",
    "\n",
    "# Step 4: Print the standard deviation of the filtered rows\n",
    "print(\"\\nTop 10 expressed genes\")\n",
    "print(top_10_means[['Mean', 'StdDev', 'Median', 'Min', 'Max']])\n",
    "\n",
    "\n",
    "DCIS_tumour_stdev = [] \n",
    "new_tumours_counts = filtered_tumour_count.copy()\n",
    "for i in range(min(10, len(new_tumours_counts))):\n",
    "    # Calculate the mean for each row\n",
    "    row_means = new_tumours_counts.mean(axis=1)\n",
    "    \n",
    "    # Calculate the standard deviation of these means\n",
    "    std_dev_of_means = row_means.std(ddof=0)\n",
    "    \n",
    "    # Append the standard deviation to the list\n",
    "    DCIS_tumour_stdev.append(std_dev_of_means)\n",
    "    \n",
    "    # Remove the row with the highest mean\n",
    "    max_mean_index = row_means.idxmax()\n",
    "    new_tumours_counts.drop(index=max_mean_index, inplace=True)\n",
    "\n",
    "# print(dataset_stats)\n",
    "\n",
    "print(DCIS_tumour_stdev)\n",
    "\n",
    "print(\"Stroma\")\n",
    "print(filtered_stroma_count.shape)\n",
    "dataset_stats = dataset_stats_generator(filtered_stroma_count, draw_zero_distribution=True, dataset_name=\"Sunnybrook Stroma\")\n",
    "print(\"Average Library Size: \", dataset_stats[0])\n",
    "print(\"Fraction of Zeroes: \", dataset_stats[1])\n",
    "print(\"Average Mean Expression: \", dataset_stats[2])\n",
    "print(\"Average Stdev Expression: \", dataset_stats[3])\n",
    "\n",
    "\n",
    "\n",
    "new_tumours_counts = filtered_stroma_count.copy()\n",
    "new_tumours_counts['Mean'] = filtered_stroma_count.mean(axis=1)\n",
    "new_tumours_counts['StdDev'] = filtered_stroma_count.std(axis=1)\n",
    "new_tumours_counts['Min'] = filtered_stroma_count.min(axis=1)\n",
    "new_tumours_counts['Max'] = filtered_stroma_count.max(axis=1)\n",
    "\n",
    "top_10_means = new_tumours_counts.nlargest(10, 'Mean')\n",
    "\n",
    "# Step 4: Print the standard deviation of the filtered rows\n",
    "print(\"\\nTop 10 expressed genes\")\n",
    "print(top_10_means[['Mean', 'StdDev', 'Min', 'Max']])\n",
    "\n",
    "\n",
    "DCIS_stroma_stdev = [] \n",
    "new_tumours_counts = filtered_stroma_count.copy()\n",
    "for i in range(min(10, len(new_tumours_counts))):\n",
    "    # Calculate the mean for each row\n",
    "    row_means = new_tumours_counts.mean(axis=1)\n",
    "    \n",
    "    # Calculate the standard deviation of these means\n",
    "    std_dev_of_means = row_means.std(ddof=0)\n",
    "    \n",
    "    # Append the standard deviation to the list\n",
    "    DCIS_stroma_stdev.append(std_dev_of_means)\n",
    "    \n",
    "    # Remove the row with the highest mean\n",
    "    max_mean_index = row_means.idxmax()\n",
    "    new_tumours_counts.drop(index=max_mean_index, inplace=True)\n",
    "\n",
    "# print(dataset_stats)\n",
    "\n",
    "print(DCIS_stroma_stdev)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Normal\")\n",
    "print(filtered_norm_count.shape)\n",
    "dataset_stats = dataset_stats_generator(filtered_norm_count, draw_zero_distribution=True, dataset_name=\"Sunnybrook Normals\")\n",
    "print(\"Average Library Size: \", dataset_stats[0])\n",
    "print(\"Fraction of Zeroes: \", dataset_stats[1])\n",
    "print(\"Average Mean Expression: \", dataset_stats[2])\n",
    "print(\"Average Stdev Expression: \", dataset_stats[3])\n",
    "\n",
    "new_tumours_counts = filtered_norm_count.copy()\n",
    "new_tumours_counts['Mean'] = filtered_norm_count.mean(axis=1)\n",
    "new_tumours_counts['StdDev'] = filtered_norm_count.std(axis=1)\n",
    "new_tumours_counts['Min'] = filtered_norm_count.min(axis=1)\n",
    "new_tumours_counts['Max'] = filtered_norm_count.max(axis=1)\n",
    "\n",
    "top_10_means = new_tumours_counts.nlargest(10, 'Mean')\n",
    "\n",
    "# Step 4: Print the standard deviation of the filtered rows\n",
    "print(\"\\nTop 10 expressed genes\")\n",
    "print(top_10_means[['Mean', 'StdDev', 'Min', 'Max']])\n",
    "\n",
    "\n",
    "DCIS_normal_stdev = [] \n",
    "new_tumours_counts = filtered_norm_count.copy()\n",
    "for i in range(min(10, len(new_tumours_counts))):\n",
    "    # Calculate the mean for each row\n",
    "    row_means = new_tumours_counts.mean(axis=1)\n",
    "    \n",
    "    # Calculate the standard deviation of these means\n",
    "    std_dev_of_means = row_means.std(ddof=0)\n",
    "    \n",
    "    # Append the standard deviation to the list\n",
    "    DCIS_normal_stdev.append(std_dev_of_means)\n",
    "    \n",
    "    # Remove the row with the highest mean\n",
    "    max_mean_index = row_means.idxmax()\n",
    "    new_tumours_counts.drop(index=max_mean_index, inplace=True)\n",
    "\n",
    "# print(dataset_stats)\n",
    "\n",
    "print(DCIS_normal_stdev)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8296ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the trimming percentage\n",
    "trim_percent = 0.01  # 1% trimming\n",
    "\n",
    "# Function to calculate trimmed mean for a row\n",
    "def trimmed_mean(row, proportion):\n",
    "    sorted_row = sorted(row)\n",
    "    n = len(sorted_row)\n",
    "    trim_count = int(n * proportion)\n",
    "    trimmed_row = sorted_row[trim_count:n-trim_count]\n",
    "    return sum(trimmed_row) / len(trimmed_row) if trimmed_row else float('nan')\n",
    "\n",
    "# Apply the trimmed mean function to each row\n",
    "trimmed_means = filtered_tumour_count.apply(lambda row: trimmed_mean(row, trim_percent), axis=1)\n",
    "\n",
    "# Calculate the mean of the trimmed means\n",
    "overall_mean = trimmed_means.mean()\n",
    "\n",
    "#print(\"Trimmed Means for Each Row:\")\n",
    "#print(trimmed_means)\n",
    "#print(\"\\nOverall Mean of Trimmed Means:\")\n",
    "#print(overall_mean)\n",
    "\n",
    "search_term = 'ENSG00000245532'\n",
    "\n",
    "\n",
    "matching_rows = [name for name in trimmed_means.index if search_term in name]\n",
    "\n",
    "print(\"Matching Row Names:\")\n",
    "for name in matching_rows:\n",
    "    print(trimmed_means.loc[name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c753e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try out making a plot. Log mean vs log variance.\n",
    "\n",
    "trim_percent = 0.00  # 1% trimming is 0.01\n",
    "\n",
    "def right_tailed_trimmed_mean_and_variance(row, proportion):\n",
    "    sorted_row = sorted(row)\n",
    "    n = len(sorted_row)\n",
    "    trim_count = int(n * proportion)\n",
    "    # Trim only from the right (highest values)\n",
    "    trimmed_row = sorted_row[:n - trim_count]\n",
    "    if not trimmed_row:\n",
    "        return float('nan'), float('nan')  # Return NaN if trimmed_row is empty\n",
    "\n",
    "    # Calculate the trimmed mean\n",
    "    mean = sum(trimmed_row) / len(trimmed_row)\n",
    "    \n",
    "    # Calculate the trimmed variance\n",
    "    if len(trimmed_row) > 1:\n",
    "        variance = sum((x - mean) ** 2 for x in trimmed_row) / (len(trimmed_row) - 1)\n",
    "    else:\n",
    "        variance = 0.0  # Variance is zero if only one data point remains\n",
    "\n",
    "    return mean, variance\n",
    "\n",
    "\n",
    "trimmed_means = []\n",
    "trimmed_variances = []\n",
    "for index, row in filtered_tumour_count.iterrows():\n",
    "    mean, variance = right_tailed_trimmed_mean_and_variance(row, trim_percent)\n",
    "    # Only include valid results\n",
    "    if not np.isnan(mean) and mean > 0 and variance > 0:\n",
    "        trimmed_means.append(mean)\n",
    "        trimmed_variances.append(variance)\n",
    "\n",
    "\n",
    "epsilon = 1e-10\n",
    "log_trimmed_means = np.log(np.array(trimmed_means) + epsilon)\n",
    "log_trimmed_variances = np.log(np.array(trimmed_variances) + epsilon)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "hb = plt.hexbin(log_trimmed_means, log_trimmed_variances, gridsize=50, cmap='viridis', mincnt=1)\n",
    "\n",
    "# Add a colorbar\n",
    "cb = plt.colorbar(hb)\n",
    "cb.set_label('Counts')\n",
    "\n",
    "# Plot the theoretical lines\n",
    "x_values = np.linspace(np.min(log_trimmed_means), np.max(log_trimmed_means), 100)\n",
    "\n",
    "# Variance equals mean (Poisson)\n",
    "y_values_poisson = x_values\n",
    "plt.plot(x_values, y_values_poisson, color='green', linestyle='--', label='Variance = Mean (Poisson)')\n",
    "\n",
    "# Variance equals mean squared\n",
    "y_values_quadratic = 2 * x_values\n",
    "plt.plot(x_values, y_values_quadratic, color='red', linestyle='--', label='Variance = Mean')\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel('Log (Mean)')\n",
    "plt.ylabel('Log (Variance)')\n",
    "dataset_name = \"Sunnybrook (DCIS)\"\n",
    "plt.title('Log(Mean) vs. Log(Variance): ' + dataset_name)\n",
    "# Legend and grid\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df24cf0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-03T21:59:39.614629Z",
     "iopub.status.busy": "2024-05-03T21:59:39.614468Z",
     "iopub.status.idle": "2024-05-03T22:01:32.089131Z",
     "shell.execute_reply": "2024-05-03T22:01:32.088137Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filt_filtered_tumour_count = filtered_tumour_count.iloc[1:5000]\n",
    "\n",
    "print(\"Our Data: Tumours\")\n",
    "#print(filtered_tumour_count.head(10))\n",
    "\n",
    "aic_values = filt_filtered_tumour_count.apply(manual_aic, axis=1)\n",
    "aic_values, nb_theta_DCIS, zinb_theta_DCIS, zinb_pi_DCIS = zip(*aic_values)\n",
    "\n",
    "AIC_top_rank_DCIS = pd.Series(aic_values).value_counts()      \n",
    "\n",
    "print(AIC_top_rank_DCIS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4dbf5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-03T22:01:32.093828Z",
     "iopub.status.busy": "2024-05-03T22:01:32.093658Z",
     "iopub.status.idle": "2024-05-03T22:13:50.992434Z",
     "shell.execute_reply": "2024-05-03T22:13:50.991530Z"
    }
   },
   "outputs": [],
   "source": [
    "#filtered_norm_count = filtered_norm_count.iloc[1:200]\n",
    "\n",
    "\n",
    "print(\"Our Data: Normal\")\n",
    "\n",
    "aic_values = filtered_norm_count.apply(manual_aic, axis=1)\n",
    "aic_values, nb_theta_DCISNorm, zinb_theta_DCISNorm, zinb_pi_DCISNorm = zip(*aic_values)\n",
    "\n",
    "AIC_top_rank_DCISNorm = pd.Series(aic_values).value_counts()       \n",
    "\n",
    "print(AIC_top_rank_DCISNorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5014be0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-03T22:13:50.994753Z",
     "iopub.status.busy": "2024-05-03T22:13:50.994586Z",
     "iopub.status.idle": "2024-05-03T22:24:53.449083Z",
     "shell.execute_reply": "2024-05-03T22:24:53.448185Z"
    }
   },
   "outputs": [],
   "source": [
    "#filtered_stroma_count = filtered_stroma_count.iloc[1:200]\n",
    "\n",
    "\n",
    "print(\"Our Data: Stroma\")\n",
    "\n",
    "aic_values = filtered_stroma_count.apply(manual_aic, axis=1)\n",
    "aic_values, nb_theta_DCISStrom, zinb_theta_DCISStrom, zinb_pi_DCISStrom = zip(*aic_values)\n",
    "\n",
    "AIC_top_rank_DCISStrom = pd.Series(aic_values).value_counts()    \n",
    "\n",
    "print(AIC_top_rank_DCISStrom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057fbf5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-03T22:24:53.457269Z",
     "iopub.status.busy": "2024-05-03T22:24:53.457133Z",
     "iopub.status.idle": "2024-05-03T22:24:53.481682Z",
     "shell.execute_reply": "2024-05-03T22:24:53.481080Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating a heatmap\n",
    "AIC_top_rank_GSE167977.name = \"GSE167977 (BC)\"\n",
    "AIC_top_rank_GSE181466.name = \"GSE181466 (TNBC)\"\n",
    "AIC_top_rank_GSE146889_tumours.name = \"GSE146889 (Colo./Endo.)\"\n",
    "AIC_top_rank_GSE146889_normals.name = \"GSE146889 (Normal)\"\n",
    "AIC_top_rank_GSE209998_ffpe.name = \"GSE209998 (BC|FFPE)\"\n",
    "AIC_top_rank_GSE209998_fresh.name = \"GSE209998 (BC|FRESH)\"\n",
    "AIC_top_rank_GSE47462_tumours.name = \"GSE47462 (BC)\"\n",
    "AIC_top_rank_GSE47462_normals.name = \"GSE47462 (Normal)\"\n",
    "AIC_top_rank_GSE120795_normals.name = \"GSE120795 (Normal)\"\n",
    "AIC_top_rank_CMI.name = \"TMBC Project\"\n",
    "AIC_top_rank_DCIS.name = \"DCIS (Tumour)\"\n",
    "AIC_top_rank_DCISNorm.name = \"DCIS (Normal)\"\n",
    "AIC_top_rank_DCISStrom.name = \"DCIS (Stroma)\"\n",
    "\n",
    "\n",
    "# lets combine the results in one large table\n",
    "# must adjust for events where one table is missing entries for a particular dist\n",
    "combined_table = pd.concat([\n",
    "    AIC_top_rank_GSE47462_tumours, AIC_top_rank_GSE47462_normals,\n",
    "    AIC_top_rank_GSE120795_normals, \n",
    "    AIC_top_rank_GSE146889_tumours, AIC_top_rank_GSE146889_normals,\n",
    "    AIC_top_rank_GSE167977, AIC_top_rank_GSE181466,\n",
    "    AIC_top_rank_GSE209998_ffpe, AIC_top_rank_GSE209998_fresh,\n",
    "    AIC_top_rank_CMI, AIC_top_rank_DCIS, AIC_top_rank_DCISNorm, AIC_top_rank_DCISStrom\n",
    "    ], axis=1)\n",
    "# print(combined_table)\n",
    "\n",
    "# change table to be based on percentages\n",
    "df_filled = combined_table.fillna(0)\n",
    "df_percent = df_filled.div(df_filled.sum(axis=0), axis=1) \n",
    "df_transposed = df_percent.T\n",
    "\n",
    "if 'ZEROES' in df_transposed.columns:\n",
    "    df_transposed = df_transposed.drop('ZEROES', axis=1)\n",
    "\n",
    "extra = \"All\"\n",
    "\n",
    "if (NB_ZINB_only == True):\n",
    "    new_order = ['NB', 'ZINB']\n",
    "    extra = \"NB_ZINB\"\n",
    "elif (no_ZI_AICs == True):\n",
    "    new_order = ['NB', 'Exponential', 'Gaussian', 'Poisson']\n",
    "    extra = \"No_ZI\"\n",
    "else:\n",
    "    new_order = ['NB', 'Exponential', 'Gaussian', 'Poisson', 'ZINB', 'ZIP']\n",
    "\n",
    "df_transposed = df_transposed[new_order]\n",
    "\n",
    "print(df_transposed.round(3))\n",
    "\n",
    "lib_adj = \"No\"\n",
    "if (adjust_for_lib):\n",
    "    lib_adj = \"Yes\"\n",
    "\n",
    "df_transposed.to_csv('/path/to/6.0.1_Third_Party_Data.Best_AIC_and_Stat_Generator.No_GLM/Percent_Best_AIC_Table.ZeroFract_' + \n",
    "                     str(express_percent_limit) + \".Trim_\" + str(trim_percent) + \".Lib_Adj_\" + str(lib_adj) + \".\" + str(extra) + \".ZI_start_0.9.csv\" , index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24c68a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-03T22:24:53.488210Z",
     "iopub.status.busy": "2024-05-03T22:24:53.488071Z",
     "iopub.status.idle": "2024-05-03T22:24:54.089610Z",
     "shell.execute_reply": "2024-05-03T22:24:54.089012Z"
    }
   },
   "outputs": [],
   "source": [
    "# lets use the table to create a heatmap\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Create the heatmap from percentage values above\n",
    "if (NB_ZINB_only == True):\n",
    "        # the NB/ZINB plot has just two rows so we should make it narrower\n",
    "        plt.figure(figsize=(3, 3.7))\n",
    "elif (no_ZI_AICs == True):\n",
    "        plt.figure(figsize=(4, 4))\n",
    "else: \n",
    "        plt.figure(figsize=(6, 4))\n",
    "\n",
    "\n",
    "\n",
    "heatmap = sns.heatmap(df_transposed, annot=True, fmt=\".2f\", cmap='crest_r', linewidths=.5,\n",
    "                      annot_kws={\"size\": 8, \"color\": 'w'},  # Set annotation text color to black\n",
    "                      cbar_kws={'shrink': 0.5, 'ticks': [0, 0.5, 1], 'format': '%.2f'})\n",
    "\n",
    "\n",
    "\n",
    "# make X/Y labels smaller\n",
    "plt.yticks(fontsize=7)\n",
    "plt.xticks(fontsize=7, rotation=60)\n",
    "\n",
    "# draw a horizontal line between certain rows\n",
    "gap_size = 3\n",
    "plt.axhline(y=2, color='honeydew', linewidth=gap_size)\n",
    "plt.axhline(y=3, color='honeydew', linewidth=gap_size)\n",
    "plt.axhline(y=5, color='honeydew', linewidth=gap_size)\n",
    "plt.axhline(y=6, color='honeydew', linewidth=gap_size)\n",
    "plt.axhline(y=7, color='honeydew', linewidth=gap_size)\n",
    "plt.axhline(y=9, color='honeydew', linewidth=gap_size)\n",
    "plt.axhline(y=10, color='honeydew', linewidth=gap_size)\n",
    "\n",
    "# wanted the color bar to display less digits\n",
    "cbar = heatmap.collections[0].colorbar\n",
    "cbar.ax.tick_params(labelsize=8)  \n",
    "cbar.set_ticks([cbar.vmin, 0, cbar.vmax])\n",
    "cbar.set_ticklabels([f'{cbar.vmin:.1f}', '0.0', f'{cbar.vmax:.1f}'])\n",
    "\n",
    "# Save and display the plot\n",
    "plt.tight_layout()\n",
    "plt.savefig('/path/to/6.0.1_Third_Party_Data.Best_AIC_and_Stat_Generator.No_GLM/Percent_Best_AIC_Table.ZeroFract_' + \n",
    "                     str(express_percent_limit) + \".Trim_\" + str(trim_percent) + \".Lib_Adj_\" + str(lib_adj) + \".\" + str(extra) +  \".ZI_start_0.9.pdf\",\n",
    "             dpi=300, bbox_inches='tight')  # dpi is dots per inch, for resolution\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Optional: Clear the figure after saving, so that future plt calls don't reuse the same figure\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c64e328",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-03T22:24:54.096045Z",
     "iopub.status.busy": "2024-05-03T22:24:54.095885Z",
     "iopub.status.idle": "2024-05-03T22:25:11.031935Z",
     "shell.execute_reply": "2024-05-03T22:25:11.031134Z"
    }
   },
   "outputs": [],
   "source": [
    "if (NB_ZINB_only is False) & (no_ZI_AICs is False):\n",
    "\n",
    "    # saving the dispersion from NB and dispersion/pi from ZINB\n",
    "    PATH = \"/path/to/6.0.1_Third_Party_Data.Best_AIC_and_Stat_Generator.No_GLM/\"\n",
    "\n",
    "    # starting with NB Theta\n",
    "    #np.savetxt(f'{PATH}nb_theta_GSE120795_normals.csv', np.array(nb_theta_GSE120795_normals)[None], delimiter=',', fmt='%d')\n",
    "    nb_theta_GSE120795_normals = pd.DataFrame([nb_theta_GSE120795_normals])\n",
    "    nb_theta_GSE120795_normals.to_csv(f'{PATH}nb_theta_GSE120795_normals.csv', index=False, header=False)\n",
    "\n",
    "    #np.savetxt(f'{PATH}nb_theta_GSE47462_tumours.csv', np.array(nb_theta_GSE47462_tumours)[None], delimiter=',', fmt='%d')\n",
    "    #np.savetxt(f'{PATH}nb_theta_GSE47462_normals.csv', np.array(nb_theta_GSE47462_normals)[None], delimiter=',', fmt='%d')\n",
    "\n",
    "    nb_theta_GSE47462_tumours = pd.DataFrame([nb_theta_GSE47462_tumours])\n",
    "    nb_theta_GSE47462_normals = pd.DataFrame([nb_theta_GSE47462_normals])\n",
    "\n",
    "    nb_theta_GSE47462_tumours.to_csv(f'{PATH}nb_theta_GSE47462_tumours.csv', index=False, header=False)\n",
    "    nb_theta_GSE47462_normals.to_csv(f'{PATH}nb_theta_GSE47462_normals.csv', index=False, header=False)\n",
    "\n",
    "    #np.savetxt(f'{PATH}nb_theta_GSE146889_tumours.csv', np.array(nb_theta_GSE146889_tumours)[None], delimiter=',', fmt='%d')\n",
    "    #np.savetxt(f'{PATH}nb_theta_GSE146889_normals.csv', np.array(nb_theta_GSE146889_normals)[None], delimiter=',', fmt='%d')\n",
    "\n",
    "    nb_theta_GSE146889_tumours = pd.DataFrame([nb_theta_GSE146889_tumours])\n",
    "    nb_theta_GSE146889_normals = pd.DataFrame([nb_theta_GSE146889_normals])\n",
    "\n",
    "    nb_theta_GSE146889_tumours.to_csv(f'{PATH}nb_theta_GSE146889_tumours.csv', index=False, header=False)\n",
    "    nb_theta_GSE146889_normals.to_csv(f'{PATH}nb_theta_GSE146889_normals.csv', index=False, header=False)\n",
    "\n",
    "    #np.savetxt(f'{PATH}nb_theta_GSE167977.csv', np.array(nb_theta_GSE167977)[None], delimiter=',', fmt='%d')\n",
    "    #np.savetxt(f'{PATH}nb_theta_GSE181466.csv', np.array(nb_theta_GSE181466)[None], delimiter=',', fmt='%d')\n",
    "    nb_theta_GSE167977 = pd.DataFrame([nb_theta_GSE167977])\n",
    "    nb_theta_GSE181466 = pd.DataFrame([nb_theta_GSE181466])\n",
    "\n",
    "    nb_theta_GSE167977.to_csv(f'{PATH}nb_theta_GSE167977.csv', index=False, header=False)\n",
    "    nb_theta_GSE181466.to_csv(f'{PATH}nb_theta_GSE181466.csv', index=False, header=False)\n",
    "\n",
    "    #np.savetxt(f'{PATH}nb_theta_GSE209998_ffpe.csv', np.array(nb_theta_GSE209998_ffpe)[None], delimiter=',', fmt='%d')\n",
    "    #np.savetxt(f'{PATH}nb_theta_GSE209998_fresh.csv', np.array(nb_theta_GSE209998_fresh)[None], delimiter=',', fmt='%d')\n",
    "\n",
    "    nb_theta_GSE209998_ffpe = pd.DataFrame([nb_theta_GSE209998_ffpe])\n",
    "    nb_theta_GSE209998_fresh = pd.DataFrame([nb_theta_GSE209998_fresh])\n",
    "\n",
    "    nb_theta_GSE209998_ffpe.to_csv(f'{PATH}nb_theta_GSE209998_ffpe.csv', index=False, header=False)\n",
    "    nb_theta_GSE209998_fresh.to_csv(f'{PATH}nb_theta_GSE209998_fresh.csv', index=False, header=False)\n",
    "\n",
    "\n",
    "    #np.savetxt(f'{PATH}nb_theta_CMI.csv', np.array(nb_theta_CMI)[None], delimiter=',', fmt='%d')\n",
    "\n",
    "    #np.savetxt(f'{PATH}nb_theta_DCIS.csv', np.array(nb_theta_DCIS)[None], delimiter=',', fmt='%d')\n",
    "    #np.savetxt(f'{PATH}nb_theta_DCISNorm.csv', np.array(nb_theta_DCISNorm)[None], delimiter=',', fmt='%d')\n",
    "    #np.savetxt(f'{PATH}nb_theta_DCISStrom.csv', np.array(nb_theta_DCISStrom)[None], delimiter=',', fmt='%d')\n",
    "    nb_theta_CMI = pd.DataFrame([nb_theta_CMI])\n",
    "\n",
    "    nb_theta_CMI.to_csv(f'{PATH}nb_theta_CMI.csv', index=False, header=False)\n",
    "\n",
    "    nb_theta_DCIS = pd.DataFrame([nb_theta_DCIS])\n",
    "    nb_theta_DCISNorm = pd.DataFrame([nb_theta_DCISNorm])\n",
    "    nb_theta_DCISStrom = pd.DataFrame([nb_theta_DCISStrom])\n",
    "\n",
    "\n",
    "    nb_theta_DCIS.to_csv(f'{PATH}nb_theta_DCIS.csv', index=False, header=False)\n",
    "    nb_theta_DCISNorm.to_csv(f'{PATH}nb_theta_DCISNorm.csv', index=False, header=False)\n",
    "    nb_theta_DCISStrom.to_csv(f'{PATH}nb_theta_DCISStrom.csv', index=False, header=False)\n",
    "\n",
    "\n",
    "\n",
    "    # Now ZINB Theta\n",
    "    #np.savetxt(f'{PATH}zinb_theta_GSE120795_normals.csv', np.array(zinb_theta_GSE120795_normals)[None], delimiter=',', fmt='%d')\n",
    "    zinb_theta_GSE120795_normals = pd.DataFrame([zinb_theta_GSE120795_normals])\n",
    "    zinb_theta_GSE120795_normals.to_csv(f'{PATH}zinb_theta_GSE120795_normals.csv', index=False, header=False)\n",
    "\n",
    "\n",
    "    #np.savetxt(f'{PATH}zinb_theta_GSE47462_tumours.csv', np.array(zinb_theta_GSE47462_tumours)[None], delimiter=',', fmt='%d')\n",
    "    #np.savetxt(f'{PATH}zinb_theta_GSE47462_normals.csv', np.array(zinb_theta_GSE47462_normals)[None], delimiter=',', fmt='%d')\n",
    "    zinb_theta_GSE47462_tumours = pd.DataFrame([zinb_theta_GSE47462_tumours])\n",
    "    zinb_theta_GSE47462_normals = pd.DataFrame([zinb_theta_GSE47462_normals])\n",
    "\n",
    "    zinb_theta_GSE47462_tumours.to_csv(f'{PATH}zinb_theta_GSE47462_tumours.csv', index=False, header=False)\n",
    "    zinb_theta_GSE47462_normals.to_csv(f'{PATH}zinb_theta_GSE47462_normals.csv', index=False, header=False)\n",
    "\n",
    "\n",
    "    #np.savetxt(f'{PATH}zinb_theta_GSE146889_tumours.csv', np.array(zinb_theta_GSE146889_tumours)[None], delimiter=',', fmt='%d')\n",
    "    #np.savetxt(f'{PATH}zinb_theta_GSE146889_normals.csv', np.array(zinb_theta_GSE146889_normals)[None], delimiter=',', fmt='%d')\n",
    "    zinb_theta_GSE146889_tumours = pd.DataFrame([zinb_theta_GSE146889_tumours])\n",
    "    zinb_theta_GSE146889_normals = pd.DataFrame([zinb_theta_GSE146889_normals])\n",
    "\n",
    "    zinb_theta_GSE146889_tumours.to_csv(f'{PATH}zinb_theta_GSE146889_tumours.csv', index=False, header=False)\n",
    "    zinb_theta_GSE146889_normals.to_csv(f'{PATH}zinb_theta_GSE146889_normals.csv', index=False, header=False)\n",
    "\n",
    "    #np.savetxt(f'{PATH}zinb_theta_GSE167977.csv', np.array(zinb_theta_GSE167977)[None], delimiter=',', fmt='%d')\n",
    "    #np.savetxt(f'{PATH}zinb_theta_GSE181466.csv', np.array(zinb_theta_GSE181466)[None], delimiter=',', fmt='%d')\n",
    "    zinb_theta_GSE167977 = pd.DataFrame([zinb_theta_GSE167977])\n",
    "    zinb_theta_GSE181466 = pd.DataFrame([zinb_theta_GSE181466])\n",
    "\n",
    "    zinb_theta_GSE167977.to_csv(f'{PATH}zinb_theta_GSE167977.csv', index=False, header=False)\n",
    "    zinb_theta_GSE181466.to_csv(f'{PATH}zinb_theta_GSE181466.csv', index=False, header=False)\n",
    "\n",
    "\n",
    "\n",
    "    #np.savetxt(f'{PATH}zinb_theta_GSE209998_ffpe.csv', np.array(zinb_theta_GSE209998_ffpe)[None], delimiter=',', fmt='%f')\n",
    "    #np.savetxt(f'{PATH}zinb_theta_GSE209998_fresh.csv', np.array(zinb_theta_GSE209998_fresh)[None], delimiter=',', fmt='%f')\n",
    "    #np.savetxt(f'{PATH}zinb_theta_CMI.csv', np.array(zinb_theta_CMI)[None], delimiter=',', fmt='%d')\n",
    "    zinb_theta_GSE209998_ffpe = pd.DataFrame([zinb_theta_GSE209998_ffpe])\n",
    "    zinb_theta_GSE209998_fresh = pd.DataFrame([zinb_theta_GSE209998_fresh])\n",
    "    zinb_theta_CMI = pd.DataFrame([zinb_theta_CMI])\n",
    "\n",
    "    zinb_theta_GSE209998_ffpe.to_csv(f'{PATH}zinb_theta_GSE209998_ffpe.csv', index=False, header=False)\n",
    "    zinb_theta_GSE209998_fresh.to_csv(f'{PATH}zinb_theta_GSE209998_fresh.csv', index=False, header=False)\n",
    "    zinb_theta_CMI.to_csv(f'{PATH}zinb_theta_CMI.csv', index=False, header=False)\n",
    "\n",
    "\n",
    "    #np.savetxt(f'{PATH}zinb_theta_DCIS.csv', np.array(zinb_theta_DCIS)[None], delimiter=',', fmt='%d')\n",
    "    #np.savetxt(f'{PATH}zinb_theta_DCISNorm.csv', np.array(zinb_theta_DCISNorm)[None], delimiter=',', fmt='%d')\n",
    "    #np.savetxt(f'{PATH}zinb_theta_DCISStrom.csv', np.array(zinb_theta_DCISStrom)[None], delimiter=',', fmt='%d')\n",
    "    zinb_theta_DCIS = pd.DataFrame([zinb_theta_DCIS])\n",
    "    zinb_theta_DCISNorm = pd.DataFrame([zinb_theta_DCISNorm])\n",
    "    zinb_theta_DCISStrom = pd.DataFrame([zinb_theta_DCISStrom])\n",
    "\n",
    "    zinb_theta_DCIS.to_csv(f'{PATH}zinb_theta_DCIS.csv', index=False, header=False)\n",
    "    zinb_theta_DCISNorm.to_csv(f'{PATH}zinb_theta_DCISNorm.csv', index=False, header=False)\n",
    "    zinb_theta_DCISStrom.to_csv(f'{PATH}zinb_theta_DCISStrom.csv', index=False, header=False)\n",
    "\n",
    "\n",
    "    # Now ZINB Pi - \n",
    "    #np.savetxt(f'{PATH}zinb_pi_GSE120795_normals.csv', np.array(zinb_pi_GSE120795_normals)[None], delimiter=',', fmt='%d')\n",
    "\n",
    "    #np.savetxt(f'{PATH}zinb_pi_GSE47462_tumours.csv', np.array(zinb_pi_GSE47462_tumours)[None], delimiter=',', fmt='%d')\n",
    "    #np.savetxt(f'{PATH}zinb_pi_GSE47462_normals.csv', np.array(zinb_pi_GSE47462_normals)[None], delimiter=',', fmt='%d')\n",
    "\n",
    "    zinb_pi_GSE120795_normals = pd.DataFrame([zinb_pi_GSE120795_normals])\n",
    "    zinb_pi_GSE47462_tumours = pd.DataFrame([zinb_pi_GSE47462_tumours])\n",
    "    zinb_pi_GSE47462_normals = pd.DataFrame([zinb_pi_GSE47462_normals])\n",
    "\n",
    "\n",
    "    zinb_pi_GSE120795_normals.to_csv(f'{PATH}zinb_pi_GSE120795_normals.csv', index=False, header=False)\n",
    "    zinb_pi_GSE47462_tumours.to_csv(f'{PATH}zinb_pi_GSE47462_tumours.csv', index=False, header=False)\n",
    "    zinb_pi_GSE47462_normals.to_csv(f'{PATH}zinb_pi_GSE47462_normals.csv', index=False, header=False)\n",
    "\n",
    "\n",
    "    #np.savetxt(f'{PATH}zinb_pi_GSE146889_tumours.csv', np.array(zinb_pi_GSE146889_tumours)[None], delimiter=',', fmt='%d')\n",
    "    #np.savetxt(f'{PATH}zinb_pi_GSE146889_normals.csv', np.array(zinb_pi_GSE146889_normals)[None], delimiter=',', fmt='%d')\n",
    "    zinb_pi_GSE146889_tumours = pd.DataFrame([zinb_pi_GSE146889_tumours])\n",
    "    zinb_pi_GSE146889_normals = pd.DataFrame([zinb_pi_GSE146889_normals])\n",
    "\n",
    "    zinb_pi_GSE146889_tumours.to_csv(f'{PATH}zinb_pi_GSE146889_tumours.csv', index=False, header=False)\n",
    "    zinb_pi_GSE146889_normals.to_csv(f'{PATH}zinb_pi_GSE146889_normals.csv', index=False, header=False)\n",
    "\n",
    "    #np.savetxt(f'{PATH}zinb_pi_GSE167977.csv', np.array(zinb_pi_GSE167977)[None], delimiter=',', fmt='%d')\n",
    "    #np.savetxt(f'{PATH}zinb_pi_GSE181466.csv', np.array(zinb_pi_GSE181466)[None], delimiter=',', fmt='%d')\n",
    "    zinb_pi_GSE167977 = pd.DataFrame([zinb_pi_GSE167977])\n",
    "    zinb_pi_GSE181466 = pd.DataFrame([zinb_pi_GSE181466])\n",
    "\n",
    "    zinb_pi_GSE167977.to_csv(f'{PATH}zinb_pi_GSE167977.csv', index=False, header=False)\n",
    "    zinb_pi_GSE181466.to_csv(f'{PATH}zinb_pi_GSE181466.csv', index=False, header=False)\n",
    "\n",
    "\n",
    "\n",
    "    #np.savetxt(f'{PATH}zinb_pi_GSE209998_ffpe.csv', np.array(zinb_pi_GSE209998_ffpe)[None], delimiter=',', fmt='%f')\n",
    "    #np.savetxt(f'{PATH}zinb_pi_GSE209998_fresh.csv', np.array(zinb_pi_GSE209998_fresh)[None], delimiter=',', fmt='%f')\n",
    "    #np.savetxt(f'{PATH}zinb_pi_CMI.csv', np.array(zinb_pi_CMI)[None], delimiter=',', fmt='%d')\n",
    "    zinb_pi_GSE209998_ffpe = pd.DataFrame([zinb_pi_GSE209998_ffpe])\n",
    "    zinb_pi_GSE209998_fresh = pd.DataFrame([zinb_pi_GSE209998_fresh])\n",
    "\n",
    "    zinb_pi_GSE209998_ffpe.to_csv(f'{PATH}zinb_pi_GSE209998_ffpe.csv', index=False, header=False)\n",
    "    zinb_pi_GSE209998_fresh.to_csv(f'{PATH}zinb_pi_GSE209998_fresh.csv', index=False, header=False)\n",
    "\n",
    "    zinb_pi_CMI = pd.DataFrame([zinb_pi_CMI])\n",
    "\n",
    "    zinb_pi_CMI.to_csv(f'{PATH}zinb_pi_CMI.csv', index=False, header=False)\n",
    "\n",
    "\n",
    "    #np.savetxt(f'{PATH}zinb_pi_DCIS.csv', np.array(zinb_pi_DCIS)[None], delimiter=',', fmt='%d')\n",
    "    #np.savetxt(f'{PATH}zinb_pi_DCISNorm.csv', np.array(zinb_pi_DCISNorm)[None], delimiter=',', fmt='%d')\n",
    "    #np.savetxt(f'{PATH}zinb_pi_DCISStrom.csv', np.array(zinb_pi_DCISStrom)[None], delimiter=',', fmt='%d')\n",
    "\n",
    "    zinb_pi_DCIS = pd.DataFrame([zinb_pi_DCIS])\n",
    "    zinb_pi_DCISNorm = pd.DataFrame([zinb_pi_DCISNorm])\n",
    "    zinb_pi_DCISStrom = pd.DataFrame([zinb_pi_DCISStrom])\n",
    "\n",
    "    zinb_pi_DCIS.to_csv(f'{PATH}zinb_pi_DCIS.csv', index=False, header=False)\n",
    "\n",
    "    zinb_pi_DCISNorm.to_csv(f'{PATH}zinb_pi_DCISNorm.csv', index=False, header=False)\n",
    "    zinb_pi_DCISStrom.to_csv(f'{PATH}zinb_pi_DCISStrom.csv', index=False, header=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9382aaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Std plot\n",
    "# amalgamate\n",
    "combined_data = list(zip(\n",
    "    GSE120795_stdev,\n",
    "    GSE146889_tumour_stdev,\n",
    "    GSE146889_normal_stdev,\n",
    "    GSE167977_stdev,\n",
    "    GSE181466_stdev,\n",
    "    GSE209998_ffpe_stdev,\n",
    "    GSE209998_ff_stdev,\n",
    "    GSE47462_tumour_stdev,\n",
    "    GSE47462_normal_stdev,\n",
    "    TMBC_stdev,\n",
    "    DCIS_tumour_stdev,\n",
    "    DCIS_stroma_stdev,\n",
    "    DCIS_normal_stdev\n",
    "    ))\n",
    "\n",
    "df = pd.DataFrame(combined_data, columns=['GSE120795', 'GSE146889 (Tumor)', 'GSE146889 (Normal)', 'GSE167977', 'GSE181466', \n",
    "                                          'GSE209998 (FFPE)', 'GSE209998 (FF)', 'GSE47462 (Tumor)', 'GSE47462 (Normal)',\n",
    "                                          'TMBC', 'Sunnybrook (DCIS)', 'Sunnybrook (Stromal)', 'Sunnybrook (Normal)'\n",
    "                                          ])\n",
    "\n",
    "plt.figure(figsize=(6, 4)) \n",
    "ax = df.plot(linewidth=2.5)  # Increase line width\n",
    "\n",
    "# Add labels and title\n",
    "\n",
    "plt.xlabel('Number of transcripts removed from ranked list')\n",
    "plt.ylabel('standard deviation of average counts per transcript (log)')\n",
    "#plt.title('Line Plot of DataFrame Columns')\n",
    "plt.yscale('log')\n",
    "plt.yticks(fontsize=14)  # Adjust the font size as needed\n",
    "plt.xticks(range(0, 10))\n",
    "plt.ylim(top=100000)\n",
    "\n",
    "plt.legend(fontsize='small', loc='upper left', bbox_to_anchor=(1, 1), handlelength=3)\n",
    "\n",
    "outpath = \"/path/to/6.0.1_Third_Party_Data.Best_AIC_and_Stat_Generator.No_GLM/\"\n",
    "filename = \"suppl_fig.stdev_reduction_by_removal.pdf\"\n",
    "plt.savefig(outpath + filename, format='pdf', dpi=600, bbox_inches='tight')\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ffpe_env_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
