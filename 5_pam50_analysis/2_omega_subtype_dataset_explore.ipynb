{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import seaborn as sns\n",
    "from scipy.stats import zscore\n",
    "from collections import Counter\n",
    "\n",
    "from matplotlib.patches import Rectangle, Patch\n",
    "\n",
    "sys.path.insert(0, '../4_preffect')\n",
    "from _config import configs\n",
    "from preffect_factory import factory\n",
    "import anndata as ad \n",
    "from _inference import( Inference )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def list_subfolders(directory):\n",
    "    subfolders = [entry.name for entry in os.scandir(directory) if entry.is_dir()]\n",
    "    return subfolders\n",
    "\n",
    "\n",
    "folder_search = f'/path/to/Single_E4000_M100_NB_XW200_DAW2000_KLW.1_DKL.1'\n",
    "\n",
    "\n",
    "all_subfolders = sorted(list_subfolders(folder_search))\n",
    "\n",
    "all_subfolders = all_subfolders[0:25]\n",
    "\n",
    "print(\"All sub-folders:\", all_subfolders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a specific run to this page. We are just interested in the first one, \n",
    "basic_M_1000_minibatch_200_epochs_1000_lr_0.001_lib_False_likelihood_NB_masking_MCAR_2_lambda_0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs['task'] = 'reinstate'\n",
    "\n",
    "pr_reinstate = {}\n",
    "pr_count = 0\n",
    "\n",
    "for dir_name in all_subfolders:\n",
    "\n",
    "    dir_name = re.sub(r'\\s+', '', dir_name)\n",
    "    full_path = folder_search + \"/\" + dir_name\n",
    "    configs['output_path'] = full_path\n",
    "    configs['cuda_device_num'] = 4\n",
    "    pr_reinstate[pr_count] = factory(task='reinstate', configs=configs, trigger_setup=True)\n",
    "    \n",
    "    #configs['input_inference_anndata_path'] = configs['input_anndata_path'] + 'test/' \n",
    "    #configs['task'] = 'impute_experiment'\n",
    "    #infy, error_masked, error_unmasked, df_subtype  = factory(task='impute_experiment', configs=configs, \n",
    "    #                                                preffect_obj=pr_reinstate, inference_key = 'endogenous',\n",
    "    #                                                error_type='mse')\n",
    "\n",
    "    configs['always_save'] = False\n",
    "\n",
    "    pr_count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_data = pr_reinstate[0]\n",
    "#print(pr_data.train_dataset.gene_names[965])\n",
    "\n",
    "# so position 965 is ERBB2 and 966 is ESR1\n",
    "configs_inf = pr_data.configs.copy()\n",
    "configs_inf['task'] = 'inference'\n",
    "inference_instance = Inference(pr_data, task='inference', inference_key = configs_inf['inference_key'], configs=configs_inf)\n",
    "inference_instance.run_inference()\n",
    "inference_instance.configs_inf['inference_key'] = 'endogenous'\n",
    "inference_instance.register_inference_run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the current clustering code in preffect_factory even work?\n",
    "- cluster_latent (seems hardcoded for `batch`)\n",
    "- cluster_counts (does nothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now cluster_counts works and only displays clustering on \\hat{mu}\n",
    "\n",
    "for loop_count, dir_name in enumerate(all_subfolders):\n",
    "    print(dir_name)\n",
    "    factory(task='cluster_counts', preffect_obj=pr_reinstate[loop_count], inference_key='endogenous', trigger_setup=False, configs=configs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster the latent space (cluster_latent)\n",
    "configs['always_save'] = False\n",
    "\n",
    "for loop_count, dir_name in enumerate(all_subfolders):\n",
    "    print(dir_name)\n",
    "    factory(task='cluster_latent', preffect_obj=pr_reinstate[loop_count], inference_key='endogenous', trigger_setup=False, configs=configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets evaluate the Mu and Theta of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm having a weird rounding issue where it just randomly doesn't work\n",
    "def truncate_to_one_decimal(arr):\n",
    "    factor = 10  # 10^1 for one decimal place\n",
    "    return np.floor(arr * factor) / factor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read PAM50 p and r file\n",
    "pam50_path = \"/path/to/our_dcis.NB_PAM50.Median.Trim_5.Subtype.csv\"\n",
    "\n",
    "df = pd.read_csv(pam50_path)\n",
    "\n",
    "df_pivot = df.pivot_table(index='Gene', columns='Subtype', values=['p','r'], aggfunc='first')\n",
    "df_pivot.reset_index(inplace=True)\n",
    "df_pivot.columns = [' '.join(col).strip() for col in df_pivot.columns.values]\n",
    "\n",
    "#print(df_pivot)\n",
    "\n",
    "# print(\"Gene Subtype GenMu MuHat(mean) MuHat(stdev)\")\n",
    "\n",
    "category_values_array, category_counts_array, category_omegas_array, category_true_omegas_array = {}, {}, {}, {}\n",
    "\n",
    "# the same information is in the AnnData input\n",
    "for loop_count, dir_name in enumerate(all_subfolders):\n",
    "    \n",
    "\n",
    "    category_values_list, category_counts_list, category_omegas_list, category_true_omegas_list = [], [], [], []\n",
    "\n",
    "    for index, row in df_pivot.iterrows():\n",
    "\n",
    "        gene = row['Gene']\n",
    "\n",
    "        inf_reinstate = pr_reinstate[loop_count].inference_dict['endogenous']\n",
    "        adata = inf_reinstate.return_counts_as_anndata()\n",
    "    \n",
    "        # continuing on, lets pull Mu/Theta for this gene\n",
    "        hat_mu = adata[0].X\n",
    "        hat_theta = adata[0].layers[\"X_hat_theta\"]\n",
    "        true_counts = adata[0].layers[\"original_counts\"]\n",
    "        \n",
    "        # lets convert true counts to omega\n",
    "        library_size = np.sum(true_counts, axis=1)\n",
    "\n",
    "        # Calculate omega (proportion of library size for each gene)\n",
    "        true_omega = true_counts / library_size[:, np.newaxis]\n",
    "            \n",
    "        omega = adata[0].layers[\"px_omega\"]\n",
    "        subtype = adata[0].obs['subtype']\n",
    "        gene_order = inf_reinstate.ds.gene_names\n",
    "\n",
    "        column_index = gene_order.index(gene)\n",
    "        gene_muhat = hat_mu[:, column_index]\n",
    "        gene_mutheta = hat_theta[:, column_index]\n",
    "\n",
    "        gene_truecounts = true_counts[:, column_index]\n",
    "\n",
    "        gene_omegas = omega[:, column_index]\n",
    "        gene_true_omegas = true_omega[:, column_index]\n",
    "\n",
    "        categories = subtype.cat.categories\n",
    "\n",
    "        # now I want to isolate the values based on the subtype in obs\n",
    "            \n",
    "        category_values = {category: gene_muhat[subtype == category] for category in categories}\n",
    "        category_counts = {category: gene_truecounts[subtype == category] for category in categories}\n",
    "        category_true_omega = {category: gene_true_omegas[subtype == category] for category in categories}\n",
    "        category_omegas = {category: gene_omegas[subtype == category] for category in categories}\n",
    "\n",
    "        category_values_list.append(category_values)\n",
    "        category_counts_list.append(category_counts)\n",
    "        category_omegas_list.append(category_omegas)\n",
    "        category_true_omegas_list.append(category_true_omega)\n",
    "\n",
    "    category_values_array[loop_count] = category_values_list\n",
    "    category_counts_array[loop_count] = category_counts_list\n",
    "    category_omegas_array[loop_count] = category_omegas_list\n",
    "    category_true_omegas_array[loop_count] = category_true_omegas_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_omega_data_list, all_category_omegas_df_list, all_category_true_omegas_df_list, normalized_true_omega_data_list, normalized_count_data_list = {}, {}, {}, {}, {}\n",
    "\n",
    "for loop_count, dir_name in enumerate(all_subfolders):\n",
    "\n",
    "    df_list_mu, df_list_count, df_list_omega, df_list_true_omega = [], [], [], []\n",
    "\n",
    "\n",
    "    categories = subtype.cat.categories\n",
    "\n",
    "    # Loop over categories to create DataFrames\n",
    "    for category in categories:\n",
    "\n",
    "        category_counts = {gene: category_counts_array[loop_count][i][category] for i, gene in enumerate(df_pivot['Gene'])}\n",
    "        df = pd.DataFrame(category_counts)\n",
    "        df['category'] = category\n",
    "        df_list_count.append(df)\n",
    "\n",
    "        category_omegas = {gene: category_omegas_array[loop_count][i][category] for i, gene in enumerate(df_pivot['Gene'])}\n",
    "        df = pd.DataFrame(category_omegas)\n",
    "        df['category'] = category\n",
    "        df_list_omega.append(df)\n",
    "\n",
    "        category_true_omegas = {gene: category_true_omegas_array[loop_count][i][category] for i, gene in enumerate(df_pivot['Gene'])}\n",
    "        df = pd.DataFrame(category_true_omegas)\n",
    "        df['category'] = category\n",
    "        df_list_true_omega.append(df)\n",
    "\n",
    "    all_category_counts_df = pd.concat(df_list_count)\n",
    "    all_category_omegas_df = pd.concat(df_list_omega)\n",
    "    all_category_true_omegas_df = pd.concat(df_list_true_omega)\n",
    "\n",
    "    # remove last column and normalize the data\n",
    "    # originally using Z-Score, but that's for normal; switch to min-max scaling?\n",
    "\n",
    "    # the input counts\n",
    "    category_order_counts = all_category_counts_df.pop(all_category_counts_df.columns[-1])\n",
    "    #normalized_count_data = all_category_counts_df.apply(lambda x: (x - x.mean()) / x.std(), axis=0)\n",
    "    normalized_count_data = (all_category_counts_df - all_category_counts_df.min()) / (all_category_counts_df.max() - all_category_counts_df.min())\n",
    "    normalized_count_data = normalized_count_data.transpose()\n",
    "\n",
    "    # true omegas\n",
    "    # normalization for omegas: Min/Max scaling (best for continous data between 0,1)\n",
    "    category_order_true = all_category_true_omegas_df.pop(all_category_true_omegas_df.columns[-1])\n",
    "\n",
    "    #normalized_true_omega_data = all_category_true_omegas_df.apply(lambda x: (x - x.mean()) / x.std(), axis=0)\n",
    "    normalized_true_omega_data = (all_category_true_omegas_df - all_category_true_omegas_df.min()) / (all_category_true_omegas_df.max() - all_category_true_omegas_df.min())\n",
    "    normalized_true_omega_data = normalized_true_omega_data.transpose()\n",
    "    all_category_true_omegas_df = all_category_true_omegas_df.transpose()\n",
    "\n",
    "    # hat omegas\n",
    "    category_order = all_category_omegas_df.pop(all_category_omegas_df.columns[-1])\n",
    "\n",
    "    # normalized_omega_data = all_category_omegas_df.apply(lambda x: (x - x.mean()) / x.std(), axis=0)\n",
    "    # min-max scaling\n",
    "    normalized_omega_data = (all_category_omegas_df - all_category_omegas_df.min()) / (all_category_omegas_df.max() - all_category_omegas_df.min())\n",
    "\n",
    "    normalized_omega_data = normalized_omega_data.transpose()\n",
    "    all_category_omegas_df = all_category_omegas_df.transpose()\n",
    "\n",
    "    # save useful info by run\n",
    "    normalized_omega_data_list[loop_count] = normalized_omega_data\n",
    "    all_category_omegas_df_list[loop_count] = all_category_omegas_df\n",
    "    all_category_true_omegas_df_list[loop_count] = all_category_true_omegas_df\n",
    "    normalized_true_omega_data_list[loop_count] = normalized_true_omega_data\n",
    "    normalized_count_data_list[loop_count] = normalized_count_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get true values of omega\n",
    "columns_to_select = ['basal_omega', 'her2_omega', 'luma_omega', 'lumb_omega', 'normal_omega']\n",
    "\n",
    "new_true_omegas_list = {}\n",
    "\n",
    "for loop_count, dir_name in enumerate(all_subfolders):\n",
    "    inf_reinstate = pr_reinstate[loop_count].inference_dict['endogenous']\n",
    "    adata = inf_reinstate.return_counts_as_anndata()\n",
    "    true_omegas = adata[0].var[columns_to_select]\n",
    "\n",
    "    true_omegas = true_omegas.rename(columns={'basal_omega': 'Basal', 'her2_omega': 'Her2', 'luma_omega': 'LumA', 'lumb_omega': 'LumB', 'normal_omega': 'Normal'})\n",
    "\n",
    "\n",
    "    # multiply by category_order to add the same amount of columns to both\n",
    "    # category_order is the same as long as the runs are from the same input dataset\n",
    "    counts = Counter(category_order.values)\n",
    "    print(counts)\n",
    "\n",
    "    # Create a new DataFrame to store the repeated columns\n",
    "    new_true_omegas = pd.DataFrame()\n",
    "    columns_to_concat = []\n",
    "    # Repeat each column based on the count of its corresponding word\n",
    "    # Repeat each column based on the count of its corresponding word\n",
    "    for column in true_omegas.columns:\n",
    "        if column in counts:\n",
    "            # Repeat the entire column `counts[column]` times\n",
    "            repeated_columns = pd.concat([true_omegas[column]] * counts[column], axis=1)\n",
    "            # Rename the columns to reflect repetition\n",
    "            repeated_columns.columns = [f'{column}_{i+1}' for i in range(counts[column])]\n",
    "            columns_to_concat.append(repeated_columns)\n",
    "\n",
    "    # Concatenate all repeated columns into a single DataFrame\n",
    "    new_true_omegas_list[loop_count] = pd.concat(columns_to_concat, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAM50genes = [\n",
    "    \"EGFR\",    \"CDH3\",\n",
    "    \"PHGDH\",    \"ACTR3B\",\n",
    "    \"FOXC1\",    \"MIA\",\n",
    "    \"MYC\",    \"FGFR4\",\n",
    "    \"MDM2\",    \"MLPH\",\n",
    "    \"KRT14\",    \"BCL2\",\n",
    "    \"SFRP1\",    \"KRT5\",\n",
    "    \"KRT17\",    \"SLC39A6\",\n",
    "    \"ESR1\",    \"CXXC5\",\n",
    "    \"BLVRA\",    \"FOXA1\",\n",
    "    \"GPR160\",    \"NAT1\",\n",
    "    \"MAPT\",    \"PGR\",\n",
    "    \"BAG1\",    \"TMEM45B\",\n",
    "    \"ERBB2\",    \"GRB7\",\n",
    "    \"MMP11\",    \"CDC20\",\n",
    "    \"MKI67\",    \"CCNE1\",\n",
    "    \"CENPF\",    \"NUF2\",\n",
    "    \"EXO1\",    \"KIF2C\",\n",
    "    \"ORC6\",    \"ANLN\",\n",
    "    \"CDC6\",    \"RRM2\",\n",
    "    \"UBE2T\",    \"NDC80\",\n",
    "    \"CEP55\",    \"MELK\",\n",
    "    \"TYMS\",    \"CCNB1\",\n",
    "    \"BIRC5\",    \"MYBL2\",\n",
    "    \"PTTG1\",    \"UBE2C\",\n",
    "]\n",
    "\n",
    "# re-arrange the gene order to match PAM50\n",
    "#normalized_true_omega_data_re = pd.DataFrame(normalized_true_omega_data, index=PAM50genes).iloc[::-1]\n",
    "\n",
    "#normalized_omega_data_re = pd.DataFrame(normalized_omega_data, index=PAM50genes).iloc[::-1]\n",
    "\n",
    "true_omega_data_re, omega_data_re = {}, {}\n",
    "\n",
    "for loop_count, dir_name in enumerate(all_subfolders):\n",
    "    true_omega_data_re[loop_count] = pd.DataFrame(new_true_omegas_list[loop_count], index=PAM50genes).iloc[::-1]\n",
    "\n",
    "    omega_data_re[loop_count] = pd.DataFrame(all_category_omegas_df_list[loop_count], index=PAM50genes).iloc[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets plot the True/PREFFECT omegas side by side\n",
    "\n",
    "# Map the vector categories to colors\n",
    "category_colors = {\n",
    "    'Basal': 'blue',\n",
    "    'Her2': 'orange',\n",
    "    'LumA': 'green',\n",
    "    'LumB': 'red',\n",
    "    'Normal': 'purple',\n",
    "}\n",
    "category_bar = category_order.map(category_colors)\n",
    "white_red_cmap = LinearSegmentedColormap.from_list('white_red', ['white', 'darkred'])\n",
    "\n",
    "for loop_count, dir_name in enumerate(all_subfolders):\n",
    "    print(dir_name)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 6))  # Adjust figsize as needed\n",
    "\n",
    "    # Plot the first heatmap\n",
    "    ax1 = sns.heatmap(\n",
    "        true_omega_data_re[loop_count],\n",
    "        cmap=white_red_cmap,\n",
    "        cbar=False,\n",
    "        xticklabels=True,\n",
    "        yticklabels=True,\n",
    "        ax=axes[0],\n",
    "        vmax=0.5\n",
    "    )\n",
    "\n",
    "    # Add the category bar to the first heatmap\n",
    "    for idx, color in enumerate(category_bar):\n",
    "        ax1.add_patch(Rectangle((idx, len(true_omega_data_re[loop_count])), 1, 1, color=color, transform=ax1.transData, clip_on=False))\n",
    "\n",
    "    # Add legend to the first heatmap\n",
    "    legend_patches = [Patch(color=color, label=category) for category, color in category_colors.items()]\n",
    "    #ax1.legend(handles=legend_patches, title='Categories', loc='upper right', bbox_to_anchor=(1.07, 1))\n",
    "\n",
    "    # Set title and labels for the first heatmap\n",
    "    ax1.set_title(r'Observed $\\Omega$ of Input Data [Min/Max Norm.]')\n",
    "    ax1.set_ylabel('Transcripts')\n",
    "    ax1.set_yticklabels(ax1.get_yticklabels(), fontsize=8)\n",
    "    ax1.set_xticks([])\n",
    "\n",
    "    # Plot the second heatmap\n",
    "    ax2 = sns.heatmap(\n",
    "        omega_data_re[loop_count],  \n",
    "        cmap=white_red_cmap,\n",
    "        cbar=True,\n",
    "        xticklabels=True,\n",
    "        yticklabels=True,\n",
    "        ax=axes[1],\n",
    "        vmax=0.5\n",
    "    )\n",
    "    for idx, color in enumerate(category_bar):\n",
    "        ax2.add_patch(Rectangle((idx, len(omega_data_re[loop_count])), 1, 1, color=color, transform=ax2.transData, clip_on=False))\n",
    "\n",
    "    # Add legend to the second heatmap\n",
    "    ax2.legend(handles=legend_patches, title='Categories', loc='upper right', bbox_to_anchor=(1.5, 1))\n",
    "\n",
    "    # Set title and labels for the second heatmap\n",
    "    plt.title(r'$\\Omega$ of PREFFECT Model [Min/Max Norm.]')# Replace with an appropriate title\n",
    "    ax2.set_ylabel('Transcripts')\n",
    "    ax2.set_yticklabels(ax2.get_yticklabels(), fontsize=8)\n",
    "    ax2.set_xticks([])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSD CALCULATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "def compute_jsd_scipy(P, Q):\n",
    "    # Ensure that P and Q are numpy arrays\n",
    "    P = np.array(P)\n",
    "    Q = np.array(Q)\n",
    "\n",
    "    # Normalize the distributions so they sum to 1\n",
    "    P = P / np.sum(P)\n",
    "    Q = Q / np.sum(Q)\n",
    "\n",
    "    # Compute Jensen-Shannon distance\n",
    "    js_distance = distance.jensenshannon(P, Q)\n",
    "    \n",
    "    # Square the Jensen-Shannon distance to get the divergence\n",
    "    js_divergence = js_distance ** 2\n",
    "    return js_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# omegas are a data frame where columns are samples\n",
    "# we want to get the average of each subtype (in the name, i.e. Basal_1)\n",
    "def get_category(col_name):\n",
    "    # For example \"Word_3\" -> \"Word\", \"DiffWord_2\" -> \"DiffWord\"\n",
    "    return re.split('_', col_name)[0]\n",
    "\n",
    "# category_order\n",
    "\n",
    "renamed_omega = (omega_data_re[loop_count])\n",
    "renamed_omega.columns = category_order\n",
    "print(renamed_omega)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for loop_count, dir_name in enumerate(all_subfolders):\n",
    "    print(dir_name)\n",
    "\n",
    "    # need to get averages of each subtype across the board\n",
    "   \n",
    "    cat_to_cols = {}\n",
    "    for col in true_omega_data_re[loop_count].columns:\n",
    "        cat = get_category(col)\n",
    "        cat_to_cols.setdefault(cat, []).append(col)\n",
    "\n",
    "    # Create a new DataFrame that only holds the row-wise means for each category\n",
    "    means_true = pd.DataFrame(index=true_omega_data_re[loop_count].index)  # same row labels (genes)\n",
    "    for cat, columns in cat_to_cols.items():\n",
    "        means_true[cat] = true_omega_data_re[loop_count][columns].mean(axis=1)\n",
    "\n",
    "    # print(means_true)\n",
    "\n",
    "    # Inferred omegas\n",
    "    renamed_omega = (omega_data_re[loop_count])\n",
    "    renamed_omega.columns = category_order\n",
    "\n",
    "    cat_to_cols = {}\n",
    "    for col in renamed_omega.columns:\n",
    "        cat = get_category(col)\n",
    "        cat_to_cols.setdefault(cat, []).append(col)\n",
    "\n",
    "    # Create a new DataFrame that only holds the row-wise means for each category\n",
    "    means_infer = pd.DataFrame(index=renamed_omega.index)  # same row labels (genes)\n",
    "    for cat, columns in cat_to_cols.items():\n",
    "        means_infer[cat] = renamed_omega[columns].mean(axis=1)\n",
    "\n",
    "    # print(means_infer)\n",
    "    \n",
    "    jsd_all = compute_jsd_scipy(means_true, means_infer)\n",
    "    print(means_infer.columns.values)\n",
    "    print(jsd_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsd_means_subtype = {\n",
    "   'delta JSD Input Vs 80 FN': [-0.0111,\n",
    "-0.0453,\n",
    "-0.0187,\n",
    "-0.0339,\n",
    "-0.016\n",
    "                                ],\n",
    "\n",
    "   'delta JSD Input Vs 80 FP': [-0.0129,\n",
    "-0.0843,\n",
    "0.008,\n",
    "0.0049,\n",
    "-0.0415,\n",
    "                                ],\n",
    "}\n",
    "\n",
    "\n",
    "jsd_std_subtype = {\n",
    "  'delta JSD Input Vs 80 FN': [0,0,0,0,0],\n",
    "  'delta JSD Input Vs 80 FP': [0,0,0,0,0],\n",
    "}\n",
    "\n",
    "\n",
    "# Define row names\n",
    "row_names = ['basal', 'her2-enriched', 'luminal A', 'luminal B', 'normal-like']\n",
    "\n",
    "# Create the DataFrame\n",
    "df_means = pd.DataFrame(jsd_means_subtype, index=row_names)\n",
    "df_stds = pd.DataFrame(jsd_std_subtype, index=row_names)\n",
    "\n",
    "# Plotting\n",
    "n_rows = len(df_means)\n",
    "bar_width = 0.35\n",
    "x = np.arange(n_rows)\n",
    "\n",
    "cmap = sns.color_palette(\"Reds\", as_cmap=True)\n",
    "n_bars = len(jsd_means_subtype)\n",
    "colors = cmap(np.linspace(0.83, 0.25, n_bars))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "error_params = dict(elinewidth=1, ecolor='grey', capsize=5)\n",
    "\n",
    "ax.bar(x - bar_width/2, df_means['delta JSD Input Vs 80 FN'], width=bar_width,\n",
    "       yerr=df_stds['delta JSD Input Vs 80 FN'], capsize=5, error_kw=error_params,\n",
    "       label=r'$\\Delta~JSD$($\\omega$, $\\hat{\\omega}$, $\\hat{\\omega}-$)', color=colors[0])\n",
    "\n",
    "ax.bar(x + bar_width/2, df_means['delta JSD Input Vs 80 FP'], width=bar_width,\n",
    "       yerr=df_stds['delta JSD Input Vs 80 FP'], capsize=5, error_kw=error_params, \n",
    "       label=r'$\\Delta~JSD$($\\omega$, $\\hat{\\omega}$, $\\hat{\\omega}+$)', color=colors[1])\n",
    "ax.axhline(y=0, color='black', linewidth=1)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(row_names)\n",
    "ax.set_xlabel('PAM50 Subtype')\n",
    "ax.set_ylabel(r'$\\Delta~JSD$($\\omega$, $\\hat{\\omega}$, $\\hat{\\omega}$[+/-])')\n",
    "#ax.set_title(r'$\\Delta~JSD$($\\omega$, $\\hat{\\omega}$): [$\\hat{\\omega}$ [Input Data] - $\\hat{\\omega}$ FN/FP]')\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Older heatmap code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further compute statistics\n",
    "for loop_count, dir_name in enumerate(all_subfolders):\n",
    "    print(dir_name)\n",
    "    types = list(set(category_order))\n",
    "\n",
    "    # Create a dictionary mapping each type to the list of column indices that have that type\n",
    "    type_to_columns = {}\n",
    "    for idx, t in enumerate(category_order):\n",
    "        type_to_columns.setdefault(t, []).append(idx)\n",
    "\n",
    "\n",
    "    row_stats_omega = pd.DataFrame(index=omega_data_re[loop_count].index)\n",
    "\n",
    "    for t in types:\n",
    "        # Get the list of column indices for this type\n",
    "        cols_indices = type_to_columns[t]\n",
    "        \n",
    "        # Select the columns from omega_data_re using iloc\n",
    "        cols_of_type = omega_data_re[loop_count].iloc[:, cols_indices]\n",
    "        \n",
    "        # Compute mean, min, max across the selected columns for each row\n",
    "        row_stats_omega[f'{t}_mean'] = cols_of_type.mean(axis=1)\n",
    "        row_stats_omega[f'{t}_min'] = cols_of_type.min(axis=1)\n",
    "        row_stats_omega[f'{t}_max'] = cols_of_type.max(axis=1)\n",
    "\n",
    "    #print(\"Mean/Min/Max Omega in Each Subtype [Inferred]:\")\n",
    "    #print(row_stats_omega.head())\n",
    "\n",
    "\n",
    "    # and now true omega; min/max should be invariant\n",
    "    type_to_columns = {}\n",
    "    for idx, t in enumerate(category_order):\n",
    "        type_to_columns.setdefault(t, []).append(idx)\n",
    "\n",
    "    row_stats_true_omega = pd.DataFrame(index=true_omega_data_re[loop_count].index)\n",
    "\n",
    "    for t in types:\n",
    "        # Get the list of column indices for this type\n",
    "        cols_indices = type_to_columns[t]\n",
    "        \n",
    "        # Select the columns from true_omega_data_re using iloc\n",
    "        cols_of_type = true_omega_data_re[loop_count].iloc[:, cols_indices]\n",
    "        \n",
    "        # Compute mean, min, max across the selected columns for each row\n",
    "        row_stats_true_omega[f'{t}_mean'] = cols_of_type.mean(axis=1)\n",
    "        row_stats_true_omega[f'{t}_min'] = cols_of_type.min(axis=1)\n",
    "        row_stats_true_omega[f'{t}_max'] = cols_of_type.max(axis=1)\n",
    "\n",
    "    #print(\"Mean/Min/Max Omega in Each Subtype [Generative]:\")\n",
    "    #print(row_stats_true_omega.head())\n",
    "    #print()\n",
    "\n",
    "    # now that we have that information, split them into separate variables\n",
    "    mean_cols = [col for col in row_stats_omega.columns if '_mean' in col]\n",
    "    min_cols = [col for col in row_stats_omega.columns if '_min' in col]\n",
    "    max_cols = [col for col in row_stats_omega.columns if '_max' in col]\n",
    "\n",
    "    mean_df = row_stats_omega[mean_cols]\n",
    "    min_df = row_stats_omega[min_cols]\n",
    "    max_df = row_stats_omega[max_cols]\n",
    "\n",
    "    mean_df.columns = [col.replace('_mean', '') for col in mean_df.columns]\n",
    "    min_df.columns = [col.replace('_min', '') for col in min_df.columns]\n",
    "    max_df.columns = [col.replace('_max', '') for col in max_df.columns]\n",
    "\n",
    "    # also get true omega means (don't need min max here)\n",
    "    mean_cols_true = [col for col in row_stats_true_omega.columns if '_mean' in col]\n",
    "    mean_df_true = row_stats_true_omega[mean_cols_true]\n",
    "    mean_df_true.columns = [col.replace('_mean', '') for col in mean_df_true.columns]\n",
    "\n",
    "    decimal_places = 4\n",
    "    annotations_df = min_df.round(decimal_places).astype(str) + '/' + max_df.round(decimal_places).astype(str)\n",
    "\n",
    "    # Set up the figure and axes\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 11))\n",
    "\n",
    "    # Since we have a small dataset, we'll use the same data for both heatmaps\n",
    "    sns.heatmap(\n",
    "        mean_df_true,\n",
    "        ax=axes[0],\n",
    "        cmap=white_red_cmap,\n",
    "        annot=mean_df_true.round(decimal_places).astype(str),\n",
    "        annot_kws={\"size\": 8},\n",
    "        fmt='',\n",
    "        linewidths=0.5,\n",
    "        linecolor='white',\n",
    "        cbar_kws = {\"shrink\": 0.5},\n",
    "        vmax=0.5\n",
    "    )\n",
    "\n",
    "    axes[0].set_title(r'Generated $\\omega$')\n",
    "    axes[0].set_xlabel('Subtype')\n",
    "    axes[0].set_ylabel('Gene')\n",
    "    axes[0].tick_params(axis='y', labelsize=10)\n",
    "\n",
    "    sns.heatmap(\n",
    "        mean_df,\n",
    "        ax=axes[1],\n",
    "        cmap=white_red_cmap,\n",
    "        #annot=annotations_df,\n",
    "        annot=mean_df.round(decimal_places).astype(str),\n",
    "        annot_kws={\"size\": 8},\n",
    "        fmt='',\n",
    "        linewidths=0.5,\n",
    "        linecolor='white',\n",
    "        cbar_kws = {\"shrink\": 0.5},\n",
    "        vmax=0.5\n",
    "    )\n",
    "\n",
    "    axes[1].set_title(r'Inferred $\\hat{\\omega}$ (mean $\\hat{\\omega}$ per subtype)')\n",
    "    axes[1].set_xlabel('Subtype')\n",
    "    axes[1].tick_params(axis='y', labelsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add an MRE step to my code so I can quantify these differences in a concise way\n",
    "for loop_count, dir_name in enumerate(all_subfolders):\n",
    "    print(dir_name)\n",
    "\n",
    "    subtypes = list(set(category_order))\n",
    "\n",
    "    # Create a dictionary mapping each subtype to its column indices\n",
    "    subtype_to_columns = {}\n",
    "    for idx, subtype in enumerate(category_order):\n",
    "        subtype_to_columns.setdefault(subtype, []).append(idx)\n",
    "\n",
    "    mre_stats = pd.DataFrame(index=omega_data_re[loop_count].index)\n",
    "\n",
    "    for subtype in subtypes:\n",
    "        # Get the list of column indices for this subtype\n",
    "        col_indices = subtype_to_columns[subtype]\n",
    "        \n",
    "        # Select the columns for the current subtype\n",
    "        pred_values = omega_data_re[loop_count].iloc[:, col_indices]\n",
    "        true_values = true_omega_data_re[loop_count].iloc[:, col_indices]\n",
    "\n",
    "\n",
    "        #print(true_values.loc[\"MIA\"])\n",
    "        # have to make the names the same, even though the coordinates are the same\n",
    "        pred_values.columns = true_values.columns\n",
    "\n",
    "        # Compute the Mean Relative Error for each row\n",
    "        # MRE = Mean(|predicted - true| / |true|) across the subtype's columns\n",
    "        # Add a small epsilon to true_values to avoid division by zero\n",
    "        epsilon = 1e-10\n",
    "        abs_diff = (pred_values - true_values).abs()\n",
    "        relative_error = abs_diff / (true_values.abs() + epsilon)\n",
    "\n",
    "        mre = relative_error.mean(axis=1)\n",
    "        \n",
    "        # Store the MRE values in the DataFrame\n",
    "        mre_stats[f'{subtype}_MRE'] = mre\n",
    "\n",
    "\n",
    "    #print(\"MRE statistics for each subtype and row:\")\n",
    "    #print(mre_stats.head())\n",
    "\n",
    "    # plot it\n",
    "    mre_heatmap_data = mre_stats.copy()\n",
    "\n",
    "    # Optionally, rename the index (row names) for clarity\n",
    "    mre_heatmap_data.index = [subtype.replace('_MRE', '') for subtype in mre_heatmap_data.index]\n",
    "\n",
    "    sns.set(font_scale=1.0)  # Adjust font scale if necessary\n",
    "\n",
    "    # Set up the figure\n",
    "    plt.figure(figsize=(8, 11))\n",
    "\n",
    "    # Create the heatmap\n",
    "    sns.heatmap(\n",
    "        mre_heatmap_data,\n",
    "        cmap='crest_r',\n",
    "        annot=True,\n",
    "        fmt=\".2f\",\n",
    "        linewidths=0.5,\n",
    "        linecolor='white',\n",
    "        annot_kws={\"size\": 9},\n",
    "        cbar_kws = {\"shrink\": 0.5},\n",
    "        vmax=1\n",
    "    )\n",
    "\n",
    "    plt.title(r'Mean Relative Errors (MRE) of $\\omega$ and $\\hat{\\omega}$ per Subtype')\n",
    "    plt.xlabel('Gene')\n",
    "    plt.ylabel('Subtype')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ffpe_env_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
